{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\"> 권준형, 김한진, 김영진 </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I Tabular Solution Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p1(**page 18**):\n",
    "\n",
    "+ 이 파트에서는, 강화학습 알고리즘의 핵시적인 아이디어들을 최대한 간단한 형태로 다룬다.  \n",
    "+ \"the simplest form - small enough for the approximate action-value function\"\n",
    "+ 이런 경우, 최적의 밸류 펑션과 정책을 찾기 용이하다.\n",
    "+ \"This contrasts with the approximate methods described in the next part of the book, which only find approximate solutions, but which in return can be applied effectively to much larger problems.\"\n",
    "\n",
    "<br />\n",
    "p2:\n",
    "\n",
    "+ 2챕터는 단일 상태만 존재하는 (single state) 특수한 강화학습 문제 - badit problem에 대해 다룬다.\n",
    "+ 3챕터는 이 책의 남은 부분을 통해서 다루는 - finite markov decision processes - 의 메인 아이디어 (벨만 방정식, 밸류 펑션)에 대하여 다룬다.\n",
    "\n",
    "<br />\n",
    "p3:\n",
    "\n",
    "+ 이어지는 세 챕터에서는(4~6) finite MDP를 풀기 위한 \"fundamental classes\"에 대해 다룬다 - 동적 프로그래밍, 몬테-카를로 방법, 시간차 학습법(temporal- deference).  \n",
    "+ 동작 프로그래밍은 수학적으로 개발하기 쉬우나, 완전하고 정확한 환경 모델이 필요하다(\"complete and accurate\")  \n",
    "+ 몬테-카를로 방법은 모델을 필요로 하지 않고, 개념적으로 간단하지만, \"step by step incremental computation\"에서 사용할 수 없다.  \n",
    "+ 시간차 학습 방법은 모델도 필요 없고, 증가 할 수 있지만(incremental), 다른 방법에 비해 복잡하다.  \n",
    "+ 이 방법들은  또한 효율성과 수렴 속도 측면에서도 여러 차이가 있다.\n",
    "\n",
    "<br />\n",
    "p4:\n",
    "\n",
    "+ 남은 두 챕터 (7~8)에서는 최적의 값을 얻기 위하여 세 가지 방법을 혼합(combine)하는 방법에 대해 다룬다.\n",
    "+ 7챕터에서는 몬테 카를로와 시간차 방법의 강점을 합치는 방법에 관해 다룬다.\n",
    "+ 8챕터에서는 두 학습 방법을 모델 러닝 및 플래닝 방법 (동적 프로그래밍과 같이. 즉 3가지 기본 학습 방법의 조합) 완전한 솔루션을 찾는 법에 대해 다룬다. \"for a complete and unified solution to the tabular reinforcement learning problem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2 Multi-armed bandits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p1(**page 19**):\n",
    "\n",
    "+ 강화학습이 다른 학습들과 구분되는 가장 큰 특징은 액션을 평가하는 트레이닝 셋을 사용하는 데 있다.\n",
    "+ 순수한 평가적인 피드백은 이것이 최선인지 최악인지에 관계없이, 얼마나 좋은지 평가한다.\n",
    "+ 순수한 지도적인 피드백은 실제 취한 행동에 관계없이, 올바른(보상이 높은) 행동을 하도록 지시한다.\n",
    "+ 두 종류의 피드백은 상당히 구분되는데, \"evaluative feedback depends entirely on the action taken, whereas instructive feedback is independent of the action taken.\" \n",
    "+ 또한, evaluation과 instruction이 어우러지는 사례도 있다.\n",
    "\n",
    "<br />\n",
    "p2:\n",
    "\n",
    "+ 아 챕터에서는, 간략화 된 셋팅에서의 강화학습의 평가적 측면에 대해 학습한다. 하나의 상황에서 하나의 행동만을 취하는 상황\n",
    "+ \"In this chapter we study the evaluative aspect of reinforcement learning in a simplified setting, one that does not involve learning to act in more than one situation.\"\n",
    "+ 이런 경우에 대한 연구는 평가 피드백과 지도적 피드백이 어떻게 다른지 알수 있을 것이다.\n",
    "+ \"Studying this case will enable us to see most clearly how evaluative feedback differs from, and yet can be combined with, instructive feedback.\"\n",
    "\n",
    "<br />\n",
    "p3:\n",
    "\n",
    "+ 비연관적인 피드백 평가 문제 - 간단한 버전의 n-armed bandit 문제를 통해 강화학습의 기본적인 방법 몇 가지를 소개할 것이다.\n",
    "+ \"The particular nonassociative, evaluative feedback problem that we explore is a simple version of the n-armed bandit problem\"\n",
    "\n",
    "+ 이 챕터의 끝부분에서는 우리는 연관적인 상황 - 액션에 대해 한 가지 이상의 조치가 취해지는 경우에 대해 논의함으로써 강화학습에 더 다가고자한다.\n",
    "+ \"At the end of this chapter,we take a step closer to the full reinforcement learning problem by discussing what happens when the bandit problem becomes associative, that is, when actions are taken in more than one situation\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A k-armed Bandit Problem\n",
    "\n",
    "p1:\n",
    "\n",
    "+ n-armed bandit problem에 대한 설명\n",
    "+ 우리는 반복적으로 n개의 다른 옵션/액션을 선택해야 하는 상황에 직면한다고 가정 혹은 그런 상황이다.\n",
    "+ 우리가 받을 보상은 어떤 액션을 선택하느냐에 따라 달라진다.\n",
    "+ 보상에 관련된 본문 직역 : \"각 선택한 액션에 따라 고정확률분포에서 선택한 수치적 보상을 받는다.\"\n",
    "\n",
    "additional meterial : [멀티 암드 밴딧](https://brunch.co.kr/@chris-song/62)\n",
    "+ 'armed bandit'이란 슬롯 머신을 의미한다. n-armed 문제는 카지노에 구비되어있는 여러 종류의 슬롯머신에서 잭팟이 터질 확률의 시간대가 존재하는 점에 착안한 경험적 문제에서 시작되었다. 한 번 당첨된 슬롯머신에서는 잭팟이 터질 확률이 낮고, 이에 따라 다양한 종류의 슬롯 머신을 동시에 돌릴때, n 대의 슬롯머신에서 받을 수 있는 보상을 최대화 하기 위한 고민에서 시작되었다.\n",
    "\n",
    "<br />\n",
    "p2:\n",
    "\n",
    "+ 슬롯머신에서, 하나의 레버를 당기면 그에 따른 보상이 나오는 것처럼, 하나의 액션(arm을당기면) 그에 따라 보상을 얻을 수 있음.\n",
    "+ '최고의' 레버를 고름으로써 우리는 각 선택들에 따른 보상의 합을 최대화 할 수 있다.\n",
    "\n",
    "<br />\n",
    "\n",
    "p3(**page 20**):\n",
    "\n",
    "+ n-armed problem에서 각 액션은 선택된 행동에 따라 주어진 기대되는 혹은 평균적인 보상(\"expected or mean reward\")을 받는데, 이것을 액션의 value라고 한다.\n",
    "+ 만약 당신이 액션의 value를 알고 있다면, 이 문제는 사소한 문제가 된다.(value 가 높은 액션들만 취하면 되기 때문)\n",
    "+ 이 문제에서는, 우리는 value를 확실하게는 모르지만, 추정할 수 있다고 가정한다.\n",
    "+ \"We assume that you do not know the action values with certainty, although you may have estimates\"\n",
    "\n",
    "$$ \n",
    "q_*(a) \\doteq \\mathbb{E} [R_t | A_t = a] \n",
    "$$\n",
    "\n",
    "<br />\n",
    "p4:\n",
    "\n",
    "+ 평가된 value 가 가장 큰 액션을 greedy action 이라고 함.\n",
    "+ greedy action 을 선택하는 것을 \"exploiting\" 이라고 하며, nongreedy action을 선택하는 것을 \"exploration\"이라 함.\n",
    "+ Exploitation은 현재 스텝에서 큰 보상을 얻을 수 있다.\n",
    "+ Exploration은 nongreedy action의 value를 평가함으로써, greedy action보다 더 높은 보상의 액션을 찾는 가능성이 있어서, 타임스텝을 길게 가져가는 상황에서 종합 보상(totoal reward)을 얻을 것으로 기대된다.\n",
    "+ 다만, 한번의 선택에서 exploitation과 exploration 둘 다 선택할 수는 없기 때문에 자주 '갈등' 문제가 발생한다.\n",
    "\n",
    "<br />\n",
    "\n",
    "p5:\n",
    "\n",
    "+ 어떤 경우에든, explore와 exploit 중 어떤 것을 선택하는 게 더 나은지 따지는 것은 복잡하다.\n",
    "+ 두 가지를 정교하게(sophisticated method) 조화시키는 방법은 존재하지만, 고정성 및 사전 지식에 대한 강력한 가정이 필요하거나, 책의 후반부에서 다루는 강화학습 방법을 훼손하거나 불가능하게 한다.\n",
    "\n",
    "<br />\n",
    "p6:\n",
    "\n",
    "+ 이 책에서는 exploration과 exploitation을 정교하게(sophisticated way) 맞추는데 중점을 두지 않고, 단지 균형을 잠는 것에 대해서만 생각한다.\n",
    "+ 이 챕터에서는 균형을 맞추는 간단한 방법들에 대해 알아볼 것이며, 이런 방법들은 항상 exploitation만 선택하는 경우보다 큰 보상을 받는다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Action-value Methods\n",
    "\n",
    "p1: \n",
    "\n",
    "+ True value of action as q(a)\n",
    "+ 액션의 선택과 가치에 대한 평가는 타임스텝에 따라 이루어진다. 즉, 진짜 가치를 구하기 위하여 t로 나누어 평균을 구하면 된다.  \n",
    "  ![fig_1](https://github.com/SCPSAI/Reinforcement-Learning-An-indroduction/blob/master/images/chapter02/add_2_1.PNG?raw=true)\n",
    "\n",
    "  \n",
    "  ![fig_2](https://github.com/SCPSAI/Reinforcement-Learning-An-indroduction/blob/master/images/chapter02/add_2_2.PNG?raw=true)\n",
    "  \n",
    "+ 이런 방법을 sample-average 방법이라고 하며, 액션의 가치를 측정하기 위한 최고의 방법은 아니지만, 여기서는 이 간단한 방법에 대해 알아보자.\n",
    "\n",
    "\n",
    "<br />\n",
    "\n",
    "p2(**page 21**) :\n",
    "\n",
    "+ 가장 간단한 액션 셀렉션 알고리즘은 가장 높게 측정된 value의 액션을 선택하는 것이다. \n",
    "+ 이 알고리즘은 greedy action selection 이라고 하는데, 가치 Q를 최대화 할 수 있는 액션 a를 선택한다.  \n",
    "![fig_3](https://github.com/SCPSAI/Reinforcement-Learning-An-indroduction/blob/master/images/chapter02/eq_2_2.PNG?raw=true)\n",
    "   \n",
    "+ greedy action selection은 항상 exploitation 하는 것으로, 즉시 얻을 수 있는 보상을 최대화 한다.\n",
    "\n",
    "+ 간단한 대안으로, 엡실론-그리디 방법(ε-greedy)이 있다. 일정한 확률(하이퍼 파리미터 엡실론의 값)로 탐험(exploration)하게 만드는 방법으로, 기본적으로 greedy 방법과 동일하지만 일정 확률로 랜덤 셀렉트를 하게 한다.\n",
    "+ 이 방법의 장점은 기존 그리디 방법과 다르게 탐험을 일정 횟수 시도하면서도, 횟수 t가 제한적으로 증가하여 모든 액션을 조사할 수 있는 경우, 결국 최종 종합 가치는 q(a)로 수렴하는 데 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 The 10-armed Testbed\n",
    "\n",
    "<br />\n",
    "\n",
    "p1 :\n",
    "\n",
    "+ greedy method와 ε-greedy method 비교 실험\n",
    "+ n=10 (10대의 슬롯 머신), 2000 randomly generated task, 각 action value q(a)는 평균0, 분산1의 가우시안 분포  \n",
    "액션에 따른 보상 R은 평균0, 분산 1의 가우시안 분포.\n",
    "+ Fig2.1 - 1000스텝 진행 한 결과 그래프\n",
    "\n",
    "![fig_4](https://github.com/SCPSAI/Reinforcement-Learning-An-indroduction/blob/master/images/chapter02/fig_2_1.PNG?raw=true)\n",
    "\n",
    "<br />\n",
    "\n",
    "p2(**page 22**) :\n",
    "\n",
    "+ greedy 방법과 두 ε-greedy 방법 비교 (ε=0.01 and ε=0.1)\n",
    "+ 위의 그래프에서는 경험을 많이 할수록 (진행이 많이 될 수록) 기대할 수 있는 보상이 증가하는 것을 볼 수 있다.\n",
    "+ 아래 그래프 설명은 이 못함.\n",
    "+ \"The lower graph shows that the greedy method found the optimal action in only approximately one-third of the tasks. In the other two-thirds, its initial samples of the optimal action were disappointing, and it never returned to it.\"\n",
    "\n",
    "+ ε-greedy 방법이 더 결과가 좋은데, 이는 탐험을 통해 더 나은 옵티멀 액션을 찾을 기회가 있기 때문이다.\n",
    "+ ε-greedy 방법에서 , ε = 0.1인 경우에 결과가 이른 시간에 상승하여 큰 값이 나온다. 이 실험의 경우 1000까지 실험을 한 결과이기에 ε=0.1인 경우가 더 우수하게 보이지만, 실험을 계속하면 ε=0.01인 경우가 두 값에서 더 우수한 결과를 보인다고 한다.\n",
    "+ 또한 시간의 경과에 따라 ε값을 줄여서 초반의 빠른 상승과 최종적으로 더 높은 값을 보이는 특성을 활용할 수 있다. \n",
    "\n",
    "<br />\n",
    "\n",
    "p3:\n",
    "\n",
    "![fig_5](https://github.com/SCPSAI/Reinforcement-Learning-An-indroduction/blob/master/images/chapter02/fig_2_2.PNG?raw=true)\n",
    "\n",
    "+ 두 방법중 어느 것이 더 나은지는 태스크에 달려있다. 예를들어, 보상의 분산이 0인 경우, greedy 방법이 가장 좋은 결과를 낼 것이다. 이와 반대로 보상의 true value가 계속 변하는 경우에는 탐색 방법이 더 좋은 결과를 얻을 것이다.\n",
    "+ 강화학습에서는 exploration과 exploitation에 균형을 맞추는 것이 필요하다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Incremental Implementation\n",
    "p1(**page 23**):\n",
    "\n",
    "+ 지금까지 논의한 action-value method는 관찰된 reward의 샘플 평균으로 action value를 추정한다.\n",
    "+ 다음으로 설명할 방법은 이러한 평균을 효율적으로 계산하는 방법에, 특히 일정한 메모리와 일정한 per-time-step, 대해 의문을 제기한다.\n",
    "\n",
    "<br />\n",
    "\n",
    "p2:\n",
    "\n",
    "+ Ri는 이 행동의 i번째 선택 후 받은 보상, Qn은 n-1번의 선택 후 action vale의 추정치\n",
    "\n",
    "\n",
    "\\begin{equation*}\n",
    "Q_n \\doteq \\frac{R_1+R_2+\\ldots+R_{n-1}}{n-1}\n",
    "\\end{equation*}\n",
    "\n",
    "+ 분명한 구현은 모든 보상의 기록을 유지한 다음 추정값이 필요할 때마다 계산을 수행하는 것이나, 시간이 지남에 따라 메모리 및 계산 요구사항 증가\n",
    "\n",
    "<br />\n",
    "\n",
    "p3(**page 24**):\n",
    "\n",
    "+ 작고 일정한 계산에 기반한 평균 업데이트 증분 공식\n",
    "\n",
    "+ 이 방식은 reward 계산을 위해 Qn과 n만 필요함\n",
    "\n",
    "![fig_7](https://github.com/SCPSAI/Reinforcement-Learning-An-indroduction/blob/master/images/chapter02/eq_2_3.PNG?raw=true)\n",
    "\n",
    "eq (2.3)\n",
    "\n",
    "<br />\n",
    "\n",
    "p4:\n",
    "\n",
    "![fig_8](https://github.com/SCPSAI/Reinforcement-Learning-An-indroduction/blob/master/images/chapter02/alg_2_4.PNG?raw=true)\n",
    "+ 위의 식은 일반적으로 다음과 같이 표현 가능\n",
    "\n",
    "\n",
    "\\begin{equation*}\n",
    "NewEstimate\\leftarrow OldEstimate+StepSize[Target-OldEstimate]\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "+ [Target-OldEstimate]는 추정치 오차(error)\n",
    "+ 위의 예시에서는 n번째 reward가 target\n",
    "\n",
    "<br />\n",
    "\n",
    "p5(**page 25**):\n",
    "\n",
    "+ StepSize는 time step마다 변경됨, action a에 대한 n번째 reward 처리 시 StepSize 파라미터는 1/n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Tracking a Nonstationary Problem\n",
    "\n",
    "p1:\n",
    "+ 지금까지 논의된 평균화 방법은 stationary bandit 문제, 즉 reward 확률이 시간이 지나도 변하지 않는 문제에 적합하다.\n",
    "+ 하지만 우리가 실제 다루는 문제들은 그 확률 분포가 유동적인 nonstationary 강화학습 문제에 직면하며, 이 때 과거의 reward보다 최신 reward에 더 많은 가중치를 부여하는 것이 합리적이다.\n",
    "+ 널리 사용되는 방법 중 하나는 constant step-size 파라미터를 사용하는 방법이 있다.\n",
    "+ 일례로, 위의 수식에서 1/n을 alpha로 대체\n",
    "\n",
    "$\n",
    "\\begin{equation*}\n",
    "Q_{n+1} \\doteq Q_n + \\alpha [R_n - Q_n], \\alpha \\in (0,1]\n",
    "\\end{equation*}\n",
    "$\n",
    "\n",
    "<br />\n",
    "\n",
    "p2:\n",
    "\n",
    "\n",
    "![fig_9](https://github.com/SCPSAI/Reinforcement-Learning-An-indroduction/blob/master/images/chapter02/eq_2_6.PNG?raw=true)\n",
    "\n",
    "+ weight의 합이 항상 1이 되기 때문에 weighted average라고 한다.\n",
    "+ 뒤의 weight는 n-i번째까지 관찰된 reward들에 의존\n",
    "+ 1-alpha가 1보다 작으므로 Ri에 주어진 weight는 reward 수가 증가할 수록 지수적으로 감소함 -> exponential recency-weighted average\n",
    "\n",
    "<br />\n",
    "\n",
    "p3:\n",
    "\n",
    "+ 때때로 단계마다 step-size 파라미터를 변경하는 것이 편리함\n",
    "+ 단, 수렴 조건 존재\n",
    "\n",
    "\n",
    "\\begin{equation*}\n",
    "\\sum_{n=1}^\\infty \\alpha_n(a) = \\infty \\quad \\textrm{ and } \\quad \\sum_{n=1}^\\infty \\alpha_n^2(a) < \\infty\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "+ 첫번째 조건은 단계가 초기 조건이나 임의의 변동을 극복할 수 있을 만큼 충분이 크다는 것을 보장해야.\n",
    "+ 두번째 조건은 단계가 수렴할 수 있을 정도로 충분히 작아져야.\n",
    "\n",
    "<br />\n",
    "\n",
    "p4(**page 26**):\n",
    "\n",
    "+ 1/n의 경우 두 조건을 만족하지만, alpha의 경우는 두번째 조건을 만족하지 못함(converge되지 못함)\n",
    "\n",
    "<br />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 Optimistic Initial Values\n",
    "\n",
    "p1:\n",
    "\n",
    "+ 지금까지 논의한 모든 방법은 초기 action-value 추정치에 어느 정도 의존함(language of statistics에서 이러한 방법은 초기 추정지에 편향)\n",
    "+ sample average 방법은 모든 동작을 한 번 이상 선택하면 바이어스가 사라지지만, alpha가 constant한 방법의 바이어스는 영구적이지만, (2.6)에 따라 시간이 지나며 감소한다.\n",
    "+ 장점 : 어떤 수준의 보상을 기대할 수 있는지에 대한 prior knowledge 쉽게 제공 가능\n",
    "+ 단점 : 초기 추정치가 사용자가 선택해야 하는 파라미터가 됨\n",
    "\n",
    "<br />\n",
    "\n",
    "p2:\n",
    "\n",
    "+ initial action value는 exploration을 장려하는 간단한 방법으로 사용될 수 있음\n",
    "+ 예로서 10-armed 테스트에서 initial action value를 0이 아닌 5로 설정할 경우, 초기 어떠한 조치를 선택하더라도 보상은 초기 예측치보다 작음\n",
    "+ learner는 다른 action으로 전환하고 보상을 받은 뒤 \"실망\"함\n",
    "+ 결과적으로 추정값이 수렴되기 전에 모든 action들이 시도되므로 greedy action들을 선택하더라도 상당한 양의 exploration을 수행함\n",
    "\n",
    "<br />\n",
    "\n",
    "p3:\n",
    "\n",
    "\n",
    "![fig_10](https://github.com/SCPSAI/Reinforcement-Learning-An-indroduction/blob/master/images/chapter02/fig_2_3.PNG?raw=true)\n",
    "\n",
    "+ 처음에는 optimistic 방법이 더 많이 탐색하기 때문에 성능이 저하되지만, 시간이 지남에 따라 탐색이 감소하므로 성능이 향상됨을 보임\n",
    "+ 이 방법은 exploration이 본질적으로 일시적이기 때문에 nonstationary 문제에는 적합하지 않음\n",
    "+ task가 변경되어 exploration의 필요성이 새로워지면 이 방법으로 도움이 되지 않음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.7 Upper-Confidence-Bound Action Selection\n",
    "\n",
    "p1(**page 27**):\n",
    "\n",
    "- action-value estimates의 정확도에 대한 불확실성이 항상 존재하기 때문에, Exploration이 필요함\n",
    "\n",
    "\n",
    "- greedly actions : estimates된 action-value중 가장 큰 값만 선택 → 다른 action이 더 좋은 결과를 나타낼 수 있음\n",
    "- $ \\epsilon $ - greedy actions : 총 행위 중 $ \\epsilon $ 만큼을 무작위 action으로 선택 → nealry greedy한 action만 선택 가능\n",
    "- **확실한 사실** : 다수의 시행을 거치면 불확실성은 줄어듬!\n",
    "\n",
    "**<font color='red'><center>action-value estimates의 불확실성 정도를 통한 Exploration을 하자!</center></font>**\n",
    "\n",
    "$$\n",
    "A_t \\doteq \\underset{a}{\\operatorname{argmax}} \\left[ Q_t(a) + c \\sqrt{ \\frac{\\ln {t}}{N_t(a)}} \\right]\n",
    "$$\n",
    "\n",
    "$ \\ln{t} $ : natural logarithm of $ t $\n",
    "\n",
    "$ N_t(a) $ : time $ t $ 까지 action $ a $가 선택된 횟수\n",
    "\n",
    "$ c > 0 $ : exploration의 정도에 대한 제어 변수\n",
    "\n",
    "<br />\n",
    "\n",
    "p2:\n",
    "\n",
    "$c \\sqrt{ \\frac{\\ln {t}}{N_t(a)}}$ : \n",
    "- denominaor : 시행 횟수 $t$에 대한 자연 로그\n",
    "- numerator : 시행 횟수 $t$동안 action $a$가 선택된 횟수\n",
    "- 즉, 많은 시행동안 action $a$가 선택되지 않으면, action $a$가 선택되도록 만듬 → Exploration\n",
    "\n",
    "![fig_2_4](https://github.com/SCPSAI/Reinforcement-Learning-An-indroduction/blob/master/images/chapter02/fig_2_4.PNG?raw=true)\n",
    "\n",
    "<br />\n",
    "p3(**page 28**):\n",
    "\n",
    "**<font color='red'>UCB의 단점</font>**\n",
    "- 좀 더 일반적인 reinforcement learning setting (다수의 state가 있는 경우 등) 으로의 확장이 어려움 (approximation 관점으로 확장될 때..)\n",
    "- nonstationary problem에서 적용하기 쉽지 않음 → 시간에 따라 action-value estimates가 달라지기 때문에 uncertanty를 줄일 수 없음!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Bandit Algorithms\n",
    "\n",
    "p1: \n",
    "\n",
    "- 2.1~2.7까지의 방법론 : action-value를 estimate하고, 이 estimate를 이용해서 action을 선택함\n",
    "\n",
    "**<font color='red'><center> action에 대한 선호도 $ H_t(a) $를 학습해서 높은 선호도를 갖는 action를 선택하자!</center></font>**\n",
    "\n",
    "$ H_t(a) $ : numerical **<i>preference</i>** for each action $a$\n",
    "\n",
    "- preference는 reward 관점에서 해석되는 것이 아닌, 각 action 간의 상대적인 선택 확률을 의미!\n",
    "\n",
    "$$\n",
    "Pr{A_t=a} \\doteq \\frac {e^{H_t(a)}} {\\sum_{b=1}^{k} e^{H_t(b)}} \\doteq \\pi_t(a)\n",
    "$$\n",
    "\n",
    "$\\pi_t(a)$ : time $t$에서의 action $a$를 선택할 확률\n",
    "\n",
    "- $t=1$에서 모든 preference $H_1(a)$는 모두 0이므로, 모든 action을 선택할 확률이 같음\n",
    "\n",
    "<br />\n",
    "\n",
    "p2(**page29**):\n",
    "\n",
    "- stochastic gradient asent를 통한 preference update\n",
    "\n",
    "\n",
    "$$\n",
    "H_{t+1}(A_t) \\doteq H_t(A_t) + \\alpha (R_t - \\bar{R}_t)(1-\\pi_t(A_t)), \n",
    "$$\n",
    "\n",
    "$$\n",
    "H_{t+1}(a) \\doteq H_t(a) - \\alpha (R_t - \\bar{R}_t)(\\pi_t(a))\n",
    "\\quad \\textrm{ for all }a \\neq A_t\n",
    "$$\n",
    "\n",
    "$ \\alpha > 0 $ : step-size parameter\n",
    "\n",
    "$ \\bar{R}_t \\in \\mathbb{R} $ : time $ t $까지의 모든 rewards의 평균 - baseline\n",
    "\n",
    "- 만약, reward가 baseline $ \\bar{R}_t $보다 크면? → action $ A_t $를 취할 확률이 높아짐\n",
    "- 만약, reward가 baseline $ \\bar{R}_t $보다 작으면? → action $ A_t $를 취할 확률이 작아짐\n",
    "\n",
    "<br />\n",
    "p3:\n",
    "\n",
    "Figure 2.5에 대한 설명 : expected reward를 mean 0가 아닌 mean +4로 설정한 상황에서의 성능 비교\n",
    "\n",
    "- mean +4는 gradient bandit algorithms에 어떠한 영향도 미치지 않음 → baseline이 같이 증가하므로?\n",
    "\n",
    "- 반면, baseline을 제거하면, 성능이 매우 떨어짐!\n",
    "\n",
    "![fig_2_5](https://github.com/SCPSAI/Reinforcement-Learning-An-indroduction/blob/master/images/chapter02/fig_2_5.PNG?raw=true)\n",
    "\n",
    "<br />\n",
    "p4:\n",
    "\n",
    "- by understanding it as a stochastic approximation to gradient ascent\n",
    "- measure of the increment’s effect is the partial derivative of this performance measure with respect to the preference :\n",
    "\n",
    "$$\n",
    "    H_{t+1}(a) \\doteq H_t(a) + \\alpha \\frac {\\partial \\mathbb{E}[R_t]} {\\partial H_t(a)}\n",
    "$$\n",
    "\n",
    "- measure of performance here is the expected reward : \n",
    "\n",
    "$$\n",
    "\\mathbb{E}[R_t] = \\sum_x \\pi_t(x)q_*(x)\n",
    "$$\n",
    "\n",
    "- $ q_*(x) $를 알 수 없기 때문에, gradient ascent를 exactly하게 구현할 수 없음\n",
    "- But! \n",
    "\n",
    "> in fact the updates of our algorithm (2.10) are equal to (2.11) in expected value, making the algorithm an instance of stochastic gradient ascent.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac {\\partial \\mathbb{E}[R_t]} {\\partial H_t(a)} &= \\frac {\\partial} {\\partial H_t(a)} \\left [ \\sum_x \\pi_t(x) q_*(x) \\right ] \\\\\n",
    "&= \\sum_x q_*(x) \\frac {\\partial \\pi_t(x)} {\\partial H_t(a)} \\\\\n",
    "&= \\sum_x (q_*(x)-B_t) \\frac {\\partial \\pi_t(x)} {\\partial H_t(a)}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "- $ B_t $ : baseline, x에 의존적이지 않고 어떤 scalar 값으로 설정 가능함\n",
    "\n",
    "> We can include a baseline here without changing the equality because the gradient sums to zero over all the actions,\n",
    "$ \\sum_x \\frac {\\partial \\pi_t(x)} {\\partial H_t(a)} = 0 $ - as $H_t(a)\n",
    "is changed, some action's probabilities go up and some go down, but the sum of the changes must be zero because the sum of the probabilities is always one.\n",
    "\n",
    "- $ \\pi_t(x)/\\pi_t(x) $를 곱함\n",
    "\n",
    "$$\n",
    "\\frac {\\partial \\mathbb{E}[R_t]} {\\partial H_t(a)} = \\sum_x \\pi_t(x)(q_*(x)-B_t) \\frac {\\partial \\pi_t(x)} {\\partial H_t(a)} / \\pi_t(x)\n",
    "$$\n",
    "\n",
    "- 위 수식은 expectation 형태의 수식으로 나타남\n",
    "\n",
    "> The equation is now in the form of an expectation, summing over all possible values x of the random variable At, then multiplying by the probability of taking those values.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac {\\partial \\mathbb{E}[R_t]} {\\partial H_t(a)} &= \\mathbb{E} \\left [ (q_*(x)-B_t) \\frac {\\partial \\pi_t(x)} {\\partial H_t(a)} / \\pi_t(x) \\right ] \\\\\n",
    "&= \\mathbb{E} \\left [ (R_t-\\bar{R}_t) \\frac {\\partial \\pi_t(x)} {\\partial H_t(a)} / \\pi_t(x) \\right ]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "- 임의의 scalar baseline $ B_t $를 $ \\bar{R}_t $로 치환\n",
    "- $ q_*(A_t) $ 를 $ R_t $로 치환\n",
    "    - $ \\mathbb{E}[R_t | A_t] = q_*(A_t) $ 이므로 치환 가능!\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac {\\partial \\mathbb{E}[R_t]} {\\partial H_t(a)} &= \\mathbb{E} [(R_t - \\bar{R}_t) \\pi_t (A_t) ( \\mathbb{1}_{a=A_t} - \\pi_t(a))/\\pi_t(A_t)] \\\\\n",
    "&= \\mathbb{E} [(R_t - \\bar{R}_t)( \\mathbb{1}_{a=A_t} - \\pi_t(a))]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "- $ \\frac {\\partial \\pi_t(x)} {\\partial H_t(a)} = \\pi_t(x)(\\mathbb{1}_{a=x}-\\pi_t(a)) $ 에 대한 증명 : \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac {\\partial \\pi_t(x)} {\\partial H_t(a)} &= \\frac {\\partial} {\\partial H_t(a)} \\pi_t(x) \\\\\n",
    "&= \\frac {\\partial} {\\partial H_t(a)} \\left [ \\frac {e^{H_t(x)}} {\\sum_{y=1}^{k} e^{H_t(y)} } \\right ] \\\\\n",
    "&= \\frac \n",
    "{\n",
    "    \\frac {\\partial e^{H_t(x)}} {\\partial H_t(a)}\n",
    "    \\sum_{y=1}^{k} e^{H_t(y)}\n",
    "    -\n",
    "    e^{H_t(x)}\n",
    "    \\frac {\\partial \\sum_{y=1}^{k} e^{H_t(y)}} {\\partial H_t(a)}\n",
    "} \n",
    "{\\left (\\sum_{y=1}^{k} e^{H_t(y)} \\right )^2} \\\\\n",
    "&= \\frac \n",
    "{\n",
    "    \\mathbb{1}_{a=x} e^{H_t(x)}\n",
    "    \\sum_{y=1}^{k} e^{H_t(y)}\n",
    "    -\n",
    "    e^{H_t(x)}\n",
    "    e^{H_t(a)}\n",
    "} \n",
    "{\\left (\\sum_{y=1}^{k} e^{H_t(y)} \\right )^2} \\\\\n",
    "&= \\frac \n",
    "{\n",
    "    \\mathbb{1}_{a=x} e^{H_t(x)}\n",
    "} \n",
    "{ \\sum_{y=1}^{k} e^{H_t(y)} }\n",
    "-\n",
    "\\frac \n",
    "{\n",
    "    e^{H_t(x)} e^{H_t(a)}\n",
    "} \n",
    "{ \\left ( \\sum_{y=1}^{k} e^{H_t(y)} \\right ) } \\\\\n",
    "&=\\mathbb{1}_{a=x} \\pi_t(x) - \\pi_t(x)\\pi_t(a) \\\\\n",
    "&= \\pi_t(x) \\left ( \\mathbb{1}_{a=x} - \\pi_t(a) \\right )\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "- 3번째 수식 유도 (by the quotient rule)\n",
    "- 4번째 수식 유도 $ \\frac {\\partial e^x}{\\partial x} = e^x $\n",
    "\n",
    "> We have just shown that the expected update of the gradient bandit algorithm is equal to the\n",
    "gradient of expected reward, and thus that the algorithm is an instance of stochastic gradient ascent. This **assures** us that the algorithm has robust convergence properties.\n",
    "\n",
    "- 위 증명으로부터 알 수 있는 사실 :\n",
    "    - baseline은 어떠한 임의의 값으로 설정해도 무방하다.\n",
    "    - baseline의 선택은 위 알고리즘 자체에는 영향을 미치지 않음\n",
    "    - But! **variance of the update**과 rate of convergence에는 영향을 미침\n",
    "    - baseline을 본 절에서 제시한 바와 같이, 받은 reward들의 평균으로 설정하는 것이 꼭 best는 아님\n",
    "        - 그러나 쉽게 적용 가능하고, practical하게 잘 동작 함..\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.9 Associative Search (Contextual Bandits)\n",
    "\n",
    "p1:\n",
    "\n",
    "- 본 장에서는 nonassociative task에 대해서만 고려하였음\n",
    "    - action과 state에 대한 연관성이 없음!\n",
    "    - task가 stationary하거나 nonstationary한 경우, single best action을 찾는 시도를 함\n",
    "- general reinforcement learning task\n",
    "    - 하나 이상의 상황(situation)이 있음\n",
    "    - 상황(situation)과 적합한 행동(action)을 매핑하는 policy를 학습하는 것을 목적으로 함\n",
    "- 본 절에서는 본 장에서 다룬 간단한 nonassociative task를 associative task로 확장하는 방법에 대한 논의함\n",
    "\n",
    "p2:\n",
    "\n",
    "- 다수의 k-armed bendit tasks들이 있고, 각 시행 $t$에서 임의의 task에 직면한다고 가정해보자!\n",
    "    - 이 문제를 single, nonstationary k-armed bendit problem으로 생각할 수 있음\n",
    "    - 반면, true action value가 천천히 변하더라도, 본 장에서 언급한 방법론들은 잘 작용하지 않음!\n",
    "- 각 task를 선택할 수 있는 policy가 있다고 가정해보자! \n",
    "    - 각 슬롯 머신에 색상이 부여되어 있고, 각 색상에 따른 task에 본 장에서 언급한 방법론들을 개별적으로 적용할 수 있음\n",
    "    - 색상에 따라 적합한 action을 선택하는 것 → policy\n",
    "\n",
    "> With the right policy you can usually do much better than you could in the absence of any information distinguishing one bandit task from another.\n",
    "\n",
    "p3(**page 32**):\n",
    "- 상기와 같은 예시를 associative search task (a.k.a contexture bandits)라 함\n",
    "    - best action을 찾기 위한 **trial-and-error learning**\n",
    "    - 상황(situation)과 행동(action)의 연관성(**association**)\n",
    "- associative search tasks는 k-armed bendit problem과 full reinforcement learning 사이의 중간에 위치하는 task임\n",
    "    - full reinforcement learning과 같이 policy를 학습\n",
    "    - 각 action은 즉각적인(immediate) reward에만 영향을 미침\n",
    "- full reinforcement learning\n",
    "    - k-armed bendit problem과 associative search tasks에 더해,\n",
    "    - action이 다음, 그다음 상황에서의 reward까지 영향을 모두 미치는 상황을 고려함!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.10 Summary\n",
    "\n",
    "p1:\n",
    "\n",
    "- 본 장에서는 exploration과 exploitation의 균형을 맞추는 간단한 방법들에 대해 소개함\n",
    "- $\\epsilon$-greedy methods\n",
    "    - 전체 시행 중 일부 time step에서 임의의 action 선택\n",
    "- Optimistic initial value\n",
    "    - 초기 action-value 값을 optimistic하게 설정함\n",
    "    - greedy methods임에도 불구하고 exploration을 할 수 있도록 함!\n",
    "- UCB\n",
    "    - 결정적으로 행동 선택(like greedy methods)\n",
    "    - 적게 선택된 행동을 선택하도록 함\n",
    "- Gradient bandit algorithms\n",
    "    - action preference를 estimate함 (action-value를 estimate하지 않음!)\n",
    "    - softmax distribution에 따른 확률적 접근 방법\n",
    "\n",
    "<br />\n",
    "\n",
    "p2: \n",
    "\n",
    "- 어떤 방법론이 가장 좋은가?\n",
    "    - 일반적으로 이러한 질문에 대한 답을 하기는 어려움\n",
    "    - 본 절에서는 본 장에서 실험한 10-amred testbed에서 각각의 방법론들에 대한 정량적 평가를 진행\n",
    "    - But! 각 방법론들은 각기 다른 파라미터들을 가지고 있음\n",
    "    - 각 방법론들의 파라미터를 각기 달리 설정하여 learning curve를 도출하여 \"parameter study\" 진행\n",
    "    - 각 파라미터의 값을 2배씩 늘려서 실험하였으며, log-scale로 그래프 표현\n",
    "    - **그래프를 통해 주목해야 할 점:** 어떠한 paramter가 가장 적합한가? parameter값의 변화에 민감한가? \n",
    "\n",
    "![fig_2_6](https://github.com/SCPSAI/Reinforcement-Learning-An-indroduction/blob/master/images/chapter02/fig_2_6.PNG?raw=true)\n",
    "\n",
    "<br />\n",
    "\n",
    "p3:\n",
    "\n",
    "- 앞서 본 장에서 소개한 방법론들은 심플하지만, 저자들은 state of the art라고 간주하고 있음\n",
    "- 더 복잡한 방법론들이 있으나, 복잡성과 가정(assumption)들은 full reinforcement learning에 확장해서 적용하기 어려움\n",
    "- Chapter 5에서는 본 장에서 소개한 방법론들을 이용해서 full reinforcement learning을 해결하는 learning methods들을 소개함\n",
    "\n",
    "<br />\n",
    "\n",
    "p4(**page 33**):\n",
    "\n",
    "- 본 장에서 살펴본 심플한 방법론들은 exploration과 exploitation의 균형 문제에 관한 완전히 만족스러운 해결책은 아님\n",
    "\n",
    "<br />\n",
    "\n",
    "p5:\n",
    "\n",
    "- **<i>Gittins indices</i>** : k-armed bandit problem에서 exploration과 exploitation의 균형 문제를 해결할 수 있는 또 다른 방법\n",
    "    - 본 장에서 제사한 bandit problem보다 일반화된 상황에서 optimal solution을 제공함\n",
    "    - 단점1 : 가능한 문제에 대한 prior distribution이 알려져 있다고 가정함\n",
    "    - 단점2 : full reinforcement learning를 향한 일반화의 어려움 (이론과 계산 문제)\n",
    "    \n",
    "<br />\n",
    "\n",
    "p6:\n",
    "\n",
    "- **<i>Bayesian methods</i>** (a.k.a posterior sampling, Thompson sampling)\n",
    "    - action-value들에 대한 초기 distribution이 알려져 있다고 가정함\n",
    "    - action-value의 distribution을 각 step마다 update함 (즉, true action-value가 stationary하다고 가정함)\n",
    "    - update computation이 매우 복잡함 (<i>conjugate priors</i>라 하는 특정 distributions에서는 간단)\n",
    "    - posterior probability에 따라 행동을 선택함\n",
    "    - 본 장에서 언급한 distribution-free 방법론들과 유사한 성능을 보임\n",
    "\n",
    "<br />\n",
    "\n",
    "p7:\n",
    "\n",
    "> In the Bayesian setting it is even conceivable to compute the optimal balance between exploration and exploitation.\n",
    "\n",
    "> One can compute for any possible action the probability of each possible immediate reward and the resultant posterior distributions over action values. This evolving distribution becomes the information state of the problem.\n",
    "\n",
    "- 1000개 time step에 대해 모든 가능한 action, 모든 action으로 인한 rewards, 모든 가능한 next action, 모든 가능한 next reward 등등을 고려할 수 있다고 가정해 보자!\n",
    "    - 위 가정을 만족할 때, 모든 가능한 사건에 대한 reward들과 probability들이 결정되면, 가능한 action을 선택할 수 있음\n",
    "    - But! 가능한 모든 경우의 수는 action의 개수와 time-step의 수에 따라 extremely rapidly하게 늘어남\n",
    "        - 위 상황 가정에서 action이 2개밖에 없다고 가정할 때, 무려 $2^{2000}$의 경우 수를 고려해야 함\n",
    "    - **approximation approach로 접근 가능함!(현재의 연구 토픽)** → Part 2에서 소개함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
