{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\"> 권준형, 김한진, 김영진 </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3 Finite Markov Decision Processes\n",
    "  \n",
    "p1(**page 37**) :  \n",
    "+ 본 챕터에서는 finite MDPs라 부르는 finite Markov decision processes로 정의된 문제에 대해 소개함\n",
    "+ finite MDPs 문제는 evaluative feedback과 associative aspect를 모두 포함하는 문제임\n",
    "    - 서로다른 상황(state)에서 서로다른 행동(action)을 선택하는 문제\n",
    "+ MDPs에서는 $ q_* (s, a) $ 혹은, optimal action selection에서의 $ v_*(s) $ 를 추정함\n",
    "    - MDPs는 action이 즉각적인 reward 뿐만 아니라, 미래의 reward에까지 영향을 미치는 문제를 형식화함\n",
    "    - tradeoff between immediate and delayed reward\n",
    "\n",
    "p2 :\n",
    "+ MDPs는 reinforcement learning problem에 대한 수학적으로 이상적인 형태(form)임\n",
    "+ *problem's mathematical structure*의 핵심적인 요소 : return, value functions, Belmlan equations\n",
    "+ finite MDPs로 정의될 수 있는 다양한 어플리케이션에 대해 소개\n",
    "+ 다른 인공지능 기술과 마찬가지로, **breadth of applicability**와 **mathematical tractability** 사이에 tarde-off 문제가 있으며, 이 장에서 이 문제와 관련된 토론에 대하여 다룰 것임\n",
    "\n",
    ">As in all of artificial intelligence, there is a tension between breadth of\n",
    "applicability and mathematical tractability. In this chapter we introduce this\n",
    "tension and discuss some of the trade-offs and challenges that it implies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3.1 The Agent-Environment Interface**\n",
    "\n",
    "p1 :\n",
    "+ MDPs란 목표를 성취하기 위해 상호작용으로부터 학습하는 문제를 straightforward framing 하는 것을 의미\n",
    "+ **agent** : 학습자 및 결정자 (\"learner and decision-maker\")로, 학습을 시킬 대상 \n",
    "+ **environment** : 에이전트와 상호작용하는 모든 것, 에이전트의 외부를 지칭함   \n",
    "    + 에이전트와 환경의 상호작용은 **state**, **action**, **reward** 세 가지로 구성됨 (fig 3.1)\n",
    "    + **reward** : 에이전트의 행동(action)에 따라 환경으로부터 받는 특정한 값의 수\n",
    "    + **state** : 환경 중에서 에이전트의 학습을 위해 고려해야 할 사항들을 지칭함\n",
    "    + **action** : 에이전트는 매 타임스텝 t마다 스테이트를 기반으로 하나의 액션을 선택해야 하며, 액션에 따라 받는 보상이 달라짐\n",
    "    \n",
    "![fig_3_1](https://github.com/SCPSAI/Reinforcement-Learning-An-indroduction/blob/master/images/chapter03/fig_3_1.PNG?raw=true)\n",
    "\n",
    "p2 :\n",
    "+ 강화학습에서, 매 타임스텝($ t = 0, 1, 2 , ... $ )마다 에이전트와 환경 사이에  **interaction** 이 발생함\n",
    "+ 각 타임스텝에서, 에이전트는 스테이트($S_t$)를 기반으로 행동($A_t$)를 선택하며, 선택의 결과로 환경으로 부터 보상($R_{t+1}$)과 새로운 스테이트($S_{t+1}$)를 받음\n",
    "+ MDP와 에이전트에 의해 발생하는 *trajectory* 혹은 *sequence*는 다음과 같이 시작함\n",
    "\n",
    "$$\n",
    "    S_0, A_0, R_1, S_1, A_1, R_2, S_2, A_2, R_3, ...\n",
    "$$\n",
    "\n",
    "p3(**page 38**) : \n",
    "\n",
    "+ *finite* MDP에서, states, actions, rewards는 모두 유한 개수의 요소로 구성됨\n",
    "+ 스테이트와 보상은 스테이트와 액션에 의해 결정되며, 이를 수식으로 나타내면 다음과 같음\n",
    "\n",
    "$$\n",
    "    p(s', r|s,a) \\doteq \\text{Pr}\\{ S_t=s', R_t=r | S_{t-1}=s, A_{t-1}=a \\} (eq 3.2)\n",
    "$$\n",
    "\n",
    "+ 위 수식에서 $s, s' \\in S$, $r \\in R$ 그리고 $a \\in A(s)$\n",
    "+ <span style=\"color:red\"> 위 수식의 점이 있는 등호($\\doteq$)는 수식이 *실제 좌변과 우변의 값이 동일하다(fact)* 는 의미가 아닌, *이전에 정의한 함수 p의 정의* 로부터 동등한 관계가 성립함을 나타낸다. </span>\n",
    "\n",
    "+ <span style=\"color:red\"> '|'은 위 수식에서는 *p*가 a와 s의 선택에 대한 확률분포임을 나타내며, 다음을 상기시킨다. </span> \n",
    "\n",
    "$$\n",
    "    \\sum_{s' \\in S} \\sum_{r \\in R} p(s', r|s,a) = 1, \\text{for all } s \\in S, a \\in A(s)\n",
    "$$\n",
    "\n",
    "p4 : \n",
    "+ <span style=\"color:red\">이 책에선 four-argument *p* function (eq 3.2)를 주로 사용하지만, 계산하고자 하는 값에 따라 다양하게 활용 가능함 </span>\n",
    "\n",
    "> The probabilities given by the four-argument function p completely characterize the dynamics of a finite MDP.\n",
    "\n",
    "+ *state-transition probabilities*\n",
    "\n",
    "$$\n",
    "    p(s', r|s,a) \\doteq \\text{Pr}\\{ S_t=s' | S_{t-1}=s, A_{t-1}=a \\} = \\sum_{r\\in R}p(s', r|s,a)\n",
    "$$\n",
    "\n",
    "+ expected reward for state-action fair\n",
    "\n",
    "$$\n",
    "    r(s,a) \\doteq E\\{ R_t | S_{t-1}=s, A_{t-1}=a \\} =  \\sum_{r \\in R}r \\sum_{s' \\in S}  p(s', r|s,a)\n",
    "$$\n",
    "\n",
    "+ expected reward for state-action-next state\n",
    "\n",
    "$$\n",
    "    r(s,a, s') \\doteq E\\{ R_t | S_{t-1}=s, A_{t-1}=a, S_t=s' \\} =  \\sum_{r \\in R}r \\frac{p(s', r|s,a)}{p(s'|s,a)}\n",
    "$$\n",
    "\n",
    "+ 일반적으로, 액션은 어떻게 할 것인지 학습하려 하는 *결정*이 될 수 있고, 상태(state)는 액션을만드는데 도움이 되는 우리가 알고있는 모든 것을 의미함 \n",
    ">In general, actions can be any decisions we want to learn how to make, and the states can be anything we can know that might be useful in making them\n",
    "\n",
    "p5(**page 39**) :\n",
    "+ MDP 프레임워크는 추상적이고 유연기 때문에 다양한 문제에 다양한 방법으로 적용될 수 있음\n",
    "    + 예를들어, 타임 스텝의 경우 고정적인 실제 시간 간격을 사용하지 않아도 된다.\n",
    "    + 액션은 전압을 조정하는 낮은 수준의 제어부터, 점심으로 뭘 먹을지나 대학에 진학할 지 등 높은 단계의 결정을 포함한다.\n",
    "    + 스테이트는 센서의 데이터부터 물체의 상징적인 설명 등 고차원적이고 추상적인 정보까지 어떤것이든 될 수 있다.\n",
    "+ 일반적으로, 액션은 우리가 *어떠한 목적을 **어떻게** 달성할지에 대한 결정*이 될 수 있고, 스테이트는 *목적을 달성하는데 도움이되는 모든 것*이 될 수 있다.\n",
    ">In general, actions can be any decisions we want to learn how to make, and the states can be anything we can know that might be useful in making them\n",
    "\n",
    "p6 : \n",
    "+ MDP 프레임워크에서, 에이전트와 환경의 경계는 로봇이나 동물에서의 물리적인 경계와는 다르다. 보통 그것보다 에이전트에 가깝게 그려진다.\n",
    "+ 로봇으로 예를들면, 센싱 하드웨어나 기계적인 연결 파츠 등은 에이전트의 외부인 환경으로 간주된다.\n",
    "+ 자연적인 학습에서는 보상이 몸 안에서 이루어지지만, 강화학습에서는 에이전트의 외부에서 계산된다.\n",
    "\n",
    "p7 :\n",
    "+ 우리가 따라야 할 일반적인 규칙에서, 에이전트가 임의로 변경할 수 없는 모든것은 에이전트의 외부인 환경으로 간주함\n",
    "+ 그렇다고 해서, 에이전트가 외부(환경)에 대해 모르는 상태(unknown)인 것은 아님\n",
    "+ 보상에 대한 연산은, 에이전트가 마주할 *task*를 정의하기 때문에,  에이전트가 변경할 수 있는 능력의 범위 밖에서 이루어 져야 하기 때문\n",
    "+ 즉, 에이전트와 환경의 경계는 에이전트의 통제의 한계(*limit of absolue control*)를 나타내는 것이지, 에이전트의 정보를 제한하는 것이 아니다.\n",
    "\n",
    "p8 :\n",
    "+ 에이전트와 환경의 경계는 고정된 것이 아니라, 목적에 따라 다르게 설정 할 수 있다.\n",
    "+ 에이전트와-환경의 경계는 스테이트, 액션, 보상이 정해진 후에 설정하기 때문에, 경계는 어떤 작업에 목적을 두었는지를 나타낸다.\n",
    ">In practice, the agent–environment boundary is determined once one has selected particular states, actions, and rewards, and thus has identified a specific decision-making task of interest.\n",
    "\n",
    "p9 :\n",
    "+ MDP 프레임워크는 상호작용을 통한 목표지향 학습 문제에 대한 추상화이다.\n",
    "+ MDP 프레임워크는 학습 과정에서 발생하는 다양한 요소들을 세가지 신호로 간략화 하는 데 목적이 있다. \n",
    "    + 에이전트의 선택 (the actions)\n",
    "    + 에이전트의 선택의 근거 (the states)\n",
    "    + 에이전트의 목표 (the rewards)\n",
    "\n",
    "p10 :\n",
    "+ 스테이트나 액션은 업무에 따라 다양하고 이를 잘 표현하는것이 학습의 성능에 강하게 영향을 준다.\n",
    "+ 그렇지만 이런 *representational choices*는 과학보다는 예술의 영역에 가깝기에, 우리는 주로 표현(representation)이 선택된 후 어떻게 학습이 동작 하는지에 초점을 둔다\n",
    "\n",
    "> Of course, the particular states and actions vary greatly from task to task, and how they are represented can strongly affect performance. In reinforcement learning, as in other kinds of learning, such representational choices are at present more art than science. In this book we offer some advice and examples regarding good ways of representing states and actions, but our primary focus is on general principles for learning how to behave once the representations have been selected.\n",
    "\n",
    "**Example 3.1 Bioreator** (**page 40**) :\n",
    "+ 생물학 반응기의 순간 온도와 교반률(리액터 내부를 섞는 빈도)를 결정하기 위해 강화학습을 사용\n",
    "    + 액션은 목표 온도나 목표 교반률을 설정하는 일이 됨\n",
    "    + 스테이트는 센서의 측정값이나 목표 화학물질의 symbolic input이 될 수 있음\n",
    "    + 보상은 유용한 화학물질의 생성량\n",
    "+ 이 경우에서 스테이트는 센서의 측정값과 심볼 입력(symbolic input)에 대한 목록(list) 혹은 벡터이며, 액션은 온도 및 교반속도로 구성된 벡터이다. 반면, 보상은 항상 하나의 숫자로 이루어진다\n",
    "\n",
    "**Example 3.2 Pick-and-Place Robot** :\n",
    "+ 로봇암을 이용한 물건 운반 반복작업에 강화학습 적용\n",
    "    + 액션은 각 조인트(기구학에서 요소간의 접점. 전기학으로 치면 노드로 볼 수 있음)의 모터에 인가되는 전압\n",
    "    + 스테이트는 접합 각도 및 속도에 관한 측정값\n",
    "    + 보상은 물체를 성공적으로 배치하면 +1\n",
    "\n",
    "+ 액션과 스테이트는 application에 따라 다양한 형태로 정의할 수 있지만, 보상은 항상 수(numerical value)로 정의되어야 한다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3.2 Goals and Rewards**\n",
    "\n",
    "p1(**page 42**) :\n",
    "+ 강화학습에서 에이전트의 목적은 *reward*를 통해 formalize 함\n",
    "+ 매 타임스텝에서, 보상 $R_t \\in ***R***$을 얻으며 에이전트의 목적은 total amount of reward를 최대화 하는데 있음\n",
    "+ 이는 다음과 같은 *reward hypothesis*로 표현 함\n",
    "> That all of what we mean by goals and purposes can be well thought of as the maximization of the expected value of the cumulative sum of a received scalar signal (called reward).\n",
    "+ reward signal을 통해 목표를 설정하는 것이 강화학습의 특징임\n",
    "\n",
    "p2 :\n",
    "+ 보상을 한 goal fomulating은 제한적으로 보일 수 있지만, 실제로는 유연하고 광범위하게 적용 가능함\n",
    "+ 예를들어, 로봇의 움직임에 관해 학습시키고자 할 때, 연구자들은 로봇의 전진에 비례하는 단계마다 보상을 설정할 수 있음\n",
    "    + 미로를 탈출해야 하는 상황에서는 타임스텝마다 -1의 보상을 제공하여 로봇이 빠르게 탈출하도록 학습시킴\n",
    "    + 빈 캔을 모으는 로봇에는 빈 캔을 모으는 데 +1, 나머지 행동에 0의 보상을 제공함으로써 많은 캔을 수집하도록 학습시킴\n",
    "\n",
    "p3 :\n",
    "+ 에이전트는 항상 보상을 극대화 하고자 함\n",
    "+ 우리가 무언가를 달성하고자 한다면 그에대한 보상을 최대화 하여 에이전트가 가장 많은 보상을 얻으면서 우리의 목적 또한 달성하게 만들어야 함\n",
    "+ 보상을 설정할 때는 우리가 달성하고자 하는것을 제대로 반영하는 것이 중요함\n",
    "+ 체스로 예를들면, 상대의 말을 죽이는 것에 보상을 설정하면 에이전트는 승패에 관계 없이 상대의 말을 죽이기만 하도록 학습하며, 이렇게 학습하면 게임의 목표인 승리를 얻기 어려워짐\n",
    "+ 수단을 reward로 설정하면 잘못된 결과를 얻게 된다\n",
    "+ 보상 신호는 에이전트가 달성해야 하는 대상이 아니라, 우리(연구자)의 목표를 에이전트가 **어떻게** 달성하게 만들 것인지를 전달함 \n",
    ">  The reward signal is your way of communicating to the robot what you want it to achieve, not how you want it achieved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3.3 Retruns and Episodes**\n",
    "\n",
    "p1 :\n",
    "+ 에이전트의 목표는 장기적인 보상의 최대화이며, 이를 **return**이라 함\n",
    "+ 리턴은 $G_t$로 나타내며, 다음과 같이 공식화 할 수 있음\n",
    "$$\n",
    "    G_t \\doteq R_{t+1} + R_{t+2} + R_{t+3} + ... + R_{T}\n",
    "$$\n",
    "+ T는 final time step\n",
    "+ 이 접근법은 *episodes*라고 불리는 최종 단계가 있는 어플리케이션들에 적합함\n",
    "    + 연속적으로 진행되며, 최종단계가 존재하는 어플리케이션\n",
    "+ 각 에피소드는 *terminal state*라는 특정한 스테이트에서 끝나고, 표준 시작 상태로 혹은 시작 상태의 표준 분포 샘플로 리셋 됨\n",
    "+ 에피소드는 승패가 갈리는 상황처럼 서로 다른 방식으로 끝나더라도 다음 에피소드는 이전의 결말과 관계없이 시작함\n",
    "+ 에피소드로 구성된 태스크를 *episodic tasks*라 하며, 우리는 *episodic task*에서 terminal state (denoted $S$)와 non-terminal state (denoted $S^+$)를 구분 해야 함\n",
    "\n",
    "p2 :\n",
    "+ 에피소드와 달리, 자연에는 끊임없이 서로 영향을 주며 진행되는 경우가 많으며, 이를 *continuing tasks*로 부름\n",
    "+ 최종 시간 단계 T는 무한으로 발산하기 때문에, 보상 자체가 무한으로 커질 수 있음\n",
    "+ 이 책에서는 개념적으론 복잡하지만 수학적으로 단순한 리턴의 정의를 사용한다\n",
    "\n",
    "p3 (**page 60**) :\n",
    "+ 감쇠의 개념을 도입함\n",
    "$$\n",
    "    G_t \\doteq R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + ... = \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1}\n",
    "$$\n",
    "+ $\\gamma$ (0 ≤ $\\gamma$ ≤ 1)는 감쇠율\n",
    "\n",
    "p4 :\n",
    "+ 감쇠율은 미래 보상의 현재 가치를 결정함\n",
    "+ k 타임스텝 후의 보상은 $\\gamma^{k-1}$배의 가치를 가짐\n",
    "+ $\\gamma$가 0일경우 에이전트는 즉각적인 보상만을 고려하여 행동하고, 미래의 보상은 고려하지 않음\n",
    "+ $\\gamma$가 1에 가까워질수록 미래 보상이 리턴에 영향을 많이 주기 때문에 에이전트는 미래의 보상을 고려하게 됨\n",
    "\n",
    "p5 : \n",
    "$$\n",
    "    G_t \\doteq R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\gamma^3 R_{t+4} + ...\n",
    "$$\n",
    "$$\n",
    "    G_t \\doteq R_{t+1} + \\gamma (R_{t+2} + \\gamma R_{t+3} + \\gamma^2 R_{t+4} + ... )\n",
    "$$\n",
    "$$\n",
    "    G_t \\doteq R_{t+1} + \\gamma G_{t+1}\n",
    "$$\n",
    "+ 이는 모든 타임스텝 t < T에서 성립하며, 이를 통해 연속적인 보상에서 리턴을 구하기 쉬워짐\n",
    "\n",
    "p6 :\n",
    "+ p3의 공식에서 리턴은 무한의 타임스텝($T = \\infty$)에서의 총합으로 정의되지만 리턴 값은 0이 아닌 유한한 수로 정의할 수 있음 (리워드가 0이 아닌 상수이며, $\\gamma < 1$)\n",
    "+ 예를들어, 보상이 항상 +1인 경우를 가정하면,\n",
    "$$\n",
    "    G_t = \\sum_{k=0}^{\\infty}\\gamma^{k} = \\frac{1}{1-\\gamma}\n",
    "$$\n",
    "**Example 3.4 : Pole-Balancing** (**page 44**)\n",
    "![fig_3_1_1](https://github.com/SCPSAI/Reinforcement-Learning-An-indroduction/blob/master/images/chapter03/fig_3_1_1.PNG?raw=true)\n",
    "+ 폴-밸런싱은 강화학습의 초기의 어플리케이션으로 사용된 과제임\n",
    "+ 트랙을 따라 움직이는 카트를 제어하여 폴이 수직으로 선 자세를 유지하는 것이 목적\n",
    "    + 폴이 주어진 각도를 벗어나거나 카트가 트랙을 이탈할 경우 이를 에러로 간주하고 이 경우 다시 수직으로 리셋함\n",
    "+ 이 문제는 두 가지 접근 방법으로 볼 수 있음\n",
    "    + 폴의 균형을 균형을 맞추기 까지 자연적으로 사건이 발생하는 *episodic task*로 간주함\n",
    "        + 이 경우 보상은 에러가 발생하지 않는 경우 매 타임스텝마다 +1, return은 실패가 발생할 때까지의 타임스텝 수로 나타낼 수 있음\n",
    "    + 감쇠를 적용하여 *continuing task*로 간주함\n",
    "        + 보상은 실패할 경우 -1, 나머지 경우에 0\n",
    "        + 각 타임 스텝에서의 보상은 $\\gamma^K$와 관계됨\n",
    "+ 어떤 경우에도, 리턴을 최대화 되려면 폴의 균형을 가능한 한 오래 유지해야 함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MDP 프레임워크는 상호작용을 통한 goal-directed learning 문제의 상당한 추상화이다.\n",
    "달성하려는 목표가 무엇이든, 에이전트와 환경 사이에서 앞 뒤로 전달되는 세가지 signal로 줄일 수 있다.\n",
    "the actions\n",
    "the states\n",
    "the rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Unified Notation For Episodic and Continuing Tasks\n",
    "\n",
    "p1(**page 44**) :\n",
    "+ 이전 섹션에서는 두 가지 종류의 강화학습(Episodic Tasks, Continuing Tasks) 작업에 대해 설명하였다.\n",
    "+ Episodic Tasks는 각 액션이 이후 에피소드에서 받은 유한한 reward 수에만 영향을 주기 때문에 수학적으로 쉽다.\n",
    "+ 이 책에서는 종종 두 가지를 모두 고려하므로, 두 경우에 대해 동시에 정확하게 표현할 수 있는 표기법을 설정하는 것이 유용하다.\n",
    "\n",
    "p2(**page 45**) :\n",
    "+ 정확한 표기를 위해 Episodic tasks에 대한 추가 표기법이 필요하다.\n",
    "\n",
    "+ 하나의 긴 시퀸스 시간 단계 대신 유한한 시간 단계로 구성된 일련의 에피소드를 고려해야 한다.\n",
    "\n",
    "+ 우리는 각 에피소드의 시간 단계를 0부터 새로 시작하며, 시간 t에서의 상태 표현인 $ S_t $ 뿐만아니라 에피소드 i의 시간 t에서의 상태 표현인 $ S_{t,i} $를 참조해야 한다.( $ A_{t,i}, R_{t,i}, \\pi_{t,i}, T_i $ 등과 유사함)\n",
    "\n",
    "p3 :\n",
    "+ episodic task와 continuing task 모두를 다루는 단일 표기법을 얻으려면 다른 규칙이 필요하다.\n",
    "+ 우리는 앞서 유한 수의 항에 대한 합계(3.7)와 무한 수의 항에 대한 합계(3.8)로 return 값을 정의했다.\n",
    "+ 이것들은 episode의 종료가 자신에게만 전환되고 0의 reward만 생성하는 특별한 absorbing state의 진입으로 간주함으로써 통일될 수 있다.\n",
    "+ 아래 state transition diagram 고려\n",
    "\n",
    "![fig_3_1_2](https://github.com/SCPSAI/Reinforcement-Learning-An-indroduction/blob/master/images/chapter03/fig_3_1_2.PNG?raw=true)\n",
    "\n",
    "+ 실선 사각형은 episode의 끝에 해당하는 특별한 absorbing state를 나타낸다.\n",
    "+ $ S_0 $에서 시작하여 보상 시퀸스를 +1,+1,+1,0,0,0..을 얻으며, 이것을 합산하면 첫 T 보상과 무산 수의 항에 대한 합계가 동일한 reward를 얻는다.\n",
    "+ 따라서, (3.8)에 따라 두 가지를 표기할 수 있다.\n",
    "\n",
    "+ 대안으로, 다음과 같이 return을 표기할 수 있다. (T=무한대 또는 $ \\gamma $=1이 될 수 있으나 둘 다는 안됨)\n",
    "\n",
    "$$\n",
    "G_t \\doteq {\\sum_{k=t+1}^{T} \\gamma^{k-t-1}} R_k\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Policies and Value Functions\n",
    "\n",
    "p1 :\n",
    "+ 거의 모든 강화학습 알고리즘에는 value function을 추정하는 것이 포함된다. value function - state들의 함수(또는 state-action pair들의 함수)로 주어진 state에 있는 것이 agent에 얼마나 좋은지 또는 주어진 state에서 주어진 action을 수행하는 것이 얼마나 좋은지\n",
    "+ \"how good\"이라는 개념은 기대되는 미래의 reward 또는 정확한 보상으로 정의된다.\n",
    "+ 물론 agent가 미래에 받을 수 있는 reward는 어떤 action을 취하는지에 달렸기 때문에, value function은 policy라고 하는 특정 행동 방식과 관련하여 정의된다.\n",
    "\n",
    "p2 :\n",
    "+ 공식적으로, policy는 각각의 가능한 action을 선택할 수 있는 상태에서의 확률로 맵핑된다.\n",
    "+ 만약 agent가 time t에서 policy $ \\pi $를 따른다면, $ \\pi(a \\mid s)$는 $S_t = s$인 경우 $A_t=a$일 확률이다.\n",
    "+ 강화학습 방법은 경험 결과에 따라 agent의 policy가 어떻게 변경되는지를 지정한다.\n",
    "\n",
    "p3(**page 46**) : \n",
    "\n",
    "+ $ v_{\\pi}(s)$로 표시되는 policy $ \\pi $ 아래의 상태 s의 값은 s로 시작하고 그 이후 $ \\pi $ 이후에 예상되는 return 값이다.\n",
    "+ MDP의 경우 공식적으로 $v_{\\pi} 를 다음과 같이 정의할 수 있다.\n",
    "\n",
    "$$\n",
    "v_{\\pi}(s) \\doteq E_{\\pi}[G_t \\mid S_t = s]  = E_{\\pi}[ \\,{\\sum_{k=0}^{\\infty} \\gamma^{k} R_{t+k+1}} \\mid S_t = s] \\,, \\textrm{ for all } s \\in \\mathbb{s}\n",
    "$$\n",
    "\n",
    "+ 여기서 $E_{\\pi}[\\cdot]$는 agent가 policy $\\pi$를 따르는 경우 임의 변수의 예상 값을 나타내며 t는 어떠한 time step이다.\n",
    "+ 우리는 $ v_{\\pi} $ 함수를 policy $ \\pi $에 대한 state-value function이라고 부른다.\n",
    "\n",
    "p4 : \n",
    "+ 유사하게, 우리는 state s에서 policy $\\pi$ 하에서 action a를 취한 값을 $q_{\\pi}(s,a)$라고 정의한다. 즉, s로부터 시작하여 action a를 취한 후 policy $\\pi$에 따라 예상되는 return.\n",
    "\n",
    "$$\n",
    "q_{\\pi}(s,a) \\doteq E_{\\pi}[G_t \\mid S_t = s, A_t = a]  = E_{\\pi}[ \\,{\\sum_{k=0}^{\\infty} \\gamma^{k} R_{t+k+1}} \\mid S_t = s, A_t = a] \n",
    "$$\n",
    "\n",
    "우리는 $q_\\pi$를 policy $\\pi$에 대한 action-value function이라고 부른다.\n",
    "\n",
    "p5 :\n",
    "+ $ V_{\\pi}$와 $q_{\\pi}$의 값은 경험으로부터 추정할 수 있다.\n",
    "+ 예를들어, agent가 policy $\\pi$를 따르고 발생한 각 state에 대해 해당 state를 따르는 실제 return의 평균을 유자하는 경우, state에 직면한 횟수가 거의 무한대에 도달하면 평균은 state의 값인 $v_{\\pi}(s)$로 수렴된다.\n",
    "+ 각 state에서 취한 각 action에 대해 별도의 평균이 유지되면, 이러한 평균은 action value인 $q_{\\pi}(s,a)$와 유사하게 수렴된다.\n",
    "\n",
    "p6 :\n",
    "+ 강화학습 및 dynamic programming 전반에 걸쳐 사용되는 value function의 기본 속성은 이미 return (3.9)에서 설정한 것과 유사한 재귀 관계를 만족한다.\n",
    "+ 어떠한 policy $\\pi$ 및 stete s에 대해 다음의 일관성 조건이 s 값과 가능한 후속 state 값 사이에 유지된다.\n",
    "\n",
    "![eq_3_3](https://github.com/SCPSAI/Reinforcement-Learning-An-indroduction/blob/master/images/chapter03/eq_3_3.PNG?raw=true)\n",
    "\n",
    "+ 여기서 action a는 set A(s)로부터 취해지고, 다음 state s'은 set S로부터 취해지고, reward r은 set R에서부터 취해짐을 암시하고 있다.\n",
    "+ 각 트리플에 대해 확률 $\\pi(a \\mid s)p(s',r \\mid s,a)$를 계산하고, 그 확률로 괄호 안의 수량에 가중치를 부여한 다음 모든 모든 가능성에 대해 합산하여 예상 값을 얻는다.\n",
    "\n",
    "p7 :\n",
    "+ Equation (3.14)는 $v_\\pi$에 대한 Bellman equation 이다.\n",
    "+ Bellman equation은 state의 값과 후속 state 값 사이의 관계를 나타낸다.\n",
    "\n",
    "![fig_3_2_2](https://github.com/SCPSAI/Reinforcement-Learning-An-indroduction/blob/master/images/chapter03/fig_3_2_2.PNG?raw=true)\n",
    "\n",
    "+ 그림에서 각 open 원은 state를 나타내고 solid 원은 state-action pair를 나타낸다.\n",
    "+ agent는 맨 위의 root node인 state s에서 시작하여 policy $\\pi$에 따라 몇 가지 action set(diagram에서는 3개)를 취할 수 있다.\n",
    "+ 이들 각각으로부터, environment는 함수 p에 의해 주어진 dynamics에 따라 reward r과 함께 다음의 여러 상태 중 하나인 s'(diagram에서는 2개)으로 응답할 수 있다.\n",
    "+ Bellman equation (3.14)는 모든 가능성에 대해 평균을 내며 발생 가능성에 따라 가중치를 부여한다.\n",
    "+ start state의 값은 예상되는 다음 state(discounted)의 값과 그에 따라 예상되는 reward의 합과 같아야 한다.\n",
    "\n",
    "p8(**page 47**) :\n",
    "+ value function $v_\\pi$는 Bellman Equation에 대한 고유한 솔루션이다.\n",
    "+ 이 장에서 Bellman Equation이 $v_\\pi$를 계산하고 근사화하고 배우는 여러가지 방법의 기초를 형성하는 방법을 보여준다.\n",
    "\n",
    "**Example 3.5** : Gridworld\n",
    "\n",
    "\n",
    "p9 :\n",
    "\n",
    "![fig_3_2](https://github.com/SCPSAI/Reinforcement-Learning-An-indroduction/blob/master/images/chapter03/fig_3_2.PNG?raw=true)\n",
    "\n",
    "+ 그림 3.2는 단순한 finite MDP의 사각형 그리드 월드를 보여준다.\n",
    "+ 그리드의 셀은 환경 상태에 해당한다.\n",
    "+ 각 셀에서 네 가지 action(북쪽, 남쪽, 동쪽 및 서쪽)이 가능하며, 이를 기반으로 agent는 그리드에서 각 방향으로 이동한다.\n",
    "+ agent가 그리드에서 이동하지 않으면 -1의 reward를 받는다.\n",
    "+ agent가 특수한 state인 A 및 B에서 이동하는 action을 제외한 다른 action들의 reward는 0이다.\n",
    "+ state A에서, 4가지 action들은 +10의 reward를 받고, A'으로 agent가 이동한다.\n",
    "+ state B에서, 4가지 action들은 +5의 reward를 받고, B'으로 agent가 이동한다.\n",
    "\n",
    "p10 :\n",
    "+ agent가 모든 state에서 동일한 확률로 모든 4가지 action을 선택한다고 가정한다.\n",
    "+ 그림 3.2의 오른쪽에서, 이 policy에 대해 $\\gamma = 0.9$인 discount reward 사례에 대한 value function인 $v_\\pi$를 보여준다.\n",
    "+ 해당 value function은 (3.4)의 linear equation을 풀어서 계산하였다.\n",
    "+ 아래쪽 가장자리 근처의 음수값을 보면, 임의의 policy 하에서 grid의 가장자리를 칠 확률이 높기 때문에 음수로 표현됨을 확인할 수 있다.\n",
    "+ state A는 이 policy 하에서 가장 좋은 state이지만, 예상 수익은 즉각적인 reward인 +10보다 작다. 이는 agent가 A에서 A'으로 이동하면서 grid의 가장자리로 갈 수 있기 때문이다.\n",
    "+ 반면, state B는 agent가 B에서 B'으로 이동하여 양수 값을 받을 수 있기 때문에 즉각적인 reward인 +5보다 큰 값으로 평가된다.\n",
    "+ B'으로부터 grid의 edge에 도달할 수 있는 예상 패널티는 A 또는 B로 넘어갈 수 있는 예상 gain에 의해 보상된다.\n",
    "\n",
    "**Example 3.6** : Golf\n",
    "\n",
    "![fig_3_3](https://github.com/SCPSAI/Reinforcement-Learning-An-indroduction/blob/master/images/chapter03/fig_3_3.PNG?raw=true)\n",
    "\n",
    "+ 강화학습 task로 골프 홀을 치기 위해 공을 홀에 닿을 때까지 각 스트로크마다 -1의 패널티(negative reward)를 계산한다.\n",
    "+ state는 공의 위치이다.\n",
    "+ state 값은 해당 위치에서 홀 까지의 스트로크 수의 negative 이다.\n",
    "+ 우리의 action은 우리가 공을 겨냥하고 스윙하는 방식과 우리가 선택한 club이다. 이 때, putter 또는 driver만 club으로 선택한다고 가정하자.\n",
    "+ 상단 그림은 항상 putter만 사용하는 policy에 대해 가능한 state value function인 $v_{putt}(s)$를 보여준다.\n",
    "+ terminal state인 in-the-hole의 값은 0이다.\n",
    "+ 그린의 어느 곳에서나 퍼팅을 할 수 있다고 가정하며, 그 state의 값은 -1이다.\n",
    "+ 그린을 벗어나면 퍼팅으로 홀에 도달할 수 없으며, 그 값이 더 크다.\n",
    "+ 퍼팅을 통해 그린에 도달할 수 있는 경우 해당 상태는 그린의 값보다 하나 더 작은 값 즉 -2이다.\n",
    "+ 간단하게 하기 위해 우리는 매우 정확하고 결정적으로 퍼팅할 수 있지만 범위가 제한되어 있다고 가정한다.\n",
    "+ 이는 그림에서 -2로 표시된 등고선을 보여준다.\n",
    "+ 해당 선과 그린 사이의 모든 위치에는 홀을 완성하기 위해 정확히 두 번의 스트로크가 필요하다.\n",
    "+ 퍼팅을 하면 모래 함정에서 벗어날 수 없으므로 값은 -∞이다.\n",
    "+ 전반적으로 티를 홀로 가져가려면 6번의 스트로크가 필요하다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 Optimal Policies and Optimal Value Functions\n",
    "\n",
    "p1(**page 49**) : \n",
    "+ 강화학습 task를 푼다는 것은 대략 장기적으로 많은 reward를 얻는 policy를 찾는 것을 의미한다.\n",
    "+ value function은 policy에 대한 부분적 순서를 정의한다.\n",
    "+ policy $\\pi$는 모든 state에서 예상 reward가 $\\pi'$ 보다 크거나 같으면 policy $\\pi'$보다 크거나 같은 것으로 정의된다.\n",
    "+ 다시 말해서, $\\pi \\geq \\pi'$ if and only if $v_\\pi(s) \\geq v_\\pi'(s) \\textrm{ for all } s \\in \\mathbb{s} $\n",
    "+ 다른 모든 policy보다 나은 policy는 항상 하나 이상 있으며, 이는 optimal policy이다.\n",
    "+ optimal policy는 하나 이상이 있을 수 있으며, $\\pi_*$로 표기한다.\n",
    "+ optimal policy들은 동일한 state-value function을 공유하며 optimal state-value function이라 부르며, $v_*$로 표기한다.\n",
    "+ 그리고 아래와 같이 정의된다.\n",
    "\n",
    "$$\n",
    "v_*(s) \\doteq \\underset{\\pi}{\\operatorname{max}}v_\\pi(s)\n",
    "$$\n",
    "\n",
    "+ 또한 optimal policy는 동일한 optimal action-value function을 공유하며 아래와 같이 정의된다.\n",
    "\n",
    "$$\n",
    "q_*(s,a) \\doteq \\underset{\\pi}{\\operatorname{max}}q_\\pi(s,a)\n",
    "$$\n",
    "\n",
    "+ state-action pair(s,a)의 경우, state s에서 action을 취한 후 optimal policy에 따라 예상 return을 제공하므로, 다음과 같이 쓸 수 있다.\n",
    "\n",
    "$$\n",
    "q_*(s,a) = E[R_{t+1} + \\gamma v_*(S_{t+1}) \\mid S_t = s, A_t = a]\n",
    "$$\n",
    "\n",
    "\n",
    "**Example 3.7 : Optimal Value Functions for Golf**\n",
    "\n",
    "p2 : \n",
    "+ 그림 3.3의 아래는 가능한 optimal action-value function $ q_*(s,driver) $ 의 등고선(윤곽선)을 보여준다.\n",
    "+ 해당 값들은 각 state의 값들로, driver로 처음 스트로크를 하고 나서 driver나 putter 중 더 좋은 쪽을 선택한 값이다.\n",
    "+ driver를 사용하면 볼을 더 멀리 칠 수 있지만, 정확도는 떨어진다.\n",
    "+ 매우 가까운 경우에만 driver를 사용하여 한 번에 hole에 도달할 수 있다.\n",
    "+ 따라서 $q_*(s,driver)$에 대한 -1 윤곽선은 그린의 작은 부분만을 cover한다.\n",
    "+ 그러나 2 번의 스트로크가 가능하면, -2에 대한 윤곽선으로 표시된 것처럼 훨씬 더 먼 곳에서 hole에 도달할 수 있다.\n",
    "+ 이 경우, 우리는 작은 -1 윤곽선 내로 drive할 필요는 없고, 그린의 어딘가로 drive해야 한다. 거기서 putter를 사용할 수 있다.\n",
    "+ optimal action-value function은 \n",
    "\n",
    "p3 :\n",
    "+ $v_*$는 policy의 value function이므로 state 값 (3.14)에 대한 Bellman equation으로 주어진 self-consistency 조건을 충족해야 한다.\n",
    "+ 그"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.7 Optimality and Approximation\n",
    "\n",
    "p1(**page 53**) : \n",
    "+ agent는 optimal policy를 잘 배우긴 하나, 실제로 이는 힘들다.\n",
    "+ 우리가 관심을 갖는 task들에 대해 극단적인 컴퓨팅 cost로만 optimal policy들을 생성할 수 있다.\n",
    "+ 앞서 논의한 바와 같이, 환경의 dynamics에 대해 완전하고 정확한 모델이 있더라도, 일반적으로 Bellman optimality equation으로 optimal policy를 간단하게 계산한다는 것은 불가능하다.\n",
    "+ 예로서, 체스와 같은 보드 게임은 인간의 경험의 매우 작은 부분에 속하지만, custom-designed 컴퓨터는 여전히 게임에 있어서의 최적의 움직임을 계산할 수 없다.\n",
    "+ agent가 직면한 이 문제의 중요한 측면은 항상 사용 가능한 계산 능력, 특히 단일 시간 당 수행할 수 있는 계산량이다.\n",
    "\n",
    "p2 : \n",
    "+ 사용 가능한 메모리도 중요한 제약 조건이다.\n",
    "+ value functions, policies 및 모델들의 근사치(approximation)를 구축하려면 종종 많은 양의 메모리가 필요하다.\n",
    "+ 한정된 유한 상태 집합의 task들에서, 각 state( 또는 state-action pair)에 대한 하나의 entry로 배열 또는 테이블을 사용하여 이러한 approximations를 구성할 수 있다.\n",
    "+ 이를 tabular case라고 하며, 해당 method들을 tabular methods라고 한다.\n",
    "+ 그러나 많은 practical한 관심사에서는 테이블에 입력할 수 있는 것보다 훨씬 더 많은 state가 있기 때문에, 이러한 경우 더 compact한 파라미터화된 함수 표현을 사용하여 해당 함수를 근사화해야 한다.\n",
    "\n",
    "p3 :\n",
    "+ 강화학습에 대한 우리의 프레임은 우리가 approximation에 정착하도록 강요한다.\n",
    "+ 그러나 유용한 approximation을 달성할 수 있는 독특한 기회도 제공한다.\n",
    "+ 예를 들어, 최적의 행동을 근사화할 때, agent가 낮은 확률로 직면하는 많은 state들이 있을 수 있다.(agent가 받는 reward의 양에 거의 영향을 미치지 않을 정도의 suboptimal의 action을 선택하는)\n",
    "+ 예로서, Tesauro의 backgammon 플레이어는 전문가와의 게임에서는 절대 발생하지 않는 board configuration에 대해 매우 나쁜 결정을 내릴 수 있지만, exceptional한 기술로 게임을 플레이한다.\n",
    "+ 강화학습의 on-line 특성은 자주 발생하지 않는 state에 대한 노력을 줄이면서, 자주 발생하는 state에 대한 올바른 결정을 내리기 위해 더 많은 노력을 기울이는 방식으로 optimal policy를 근사화할 수 있다.\n",
    "+ 이것은 MDP를 근사화하여 풀기 위한 접근법들 중 강화학습을 다른 접근법들과 구별하는 핵심 속성 중 하나다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.8 Summary\n",
    "\n",
    "p1(**page 54**):\n",
    "\n",
    "- **Reinforcement Learning** : 목표(goal)를 성취하기 위해, 어떻게 행동해야 하는지, 상호 작용을 통해 학습하는 것\n",
    "    - *agent*와 *environment*은 지속적으로 상호작용함\n",
    "    - *actions* : agents의 선택(choice)들\n",
    "    - *states* : 선택(choice)을 만들기 위한 기초(basis)\n",
    "    - *rewards* : 선택(choice)을 평가하기 위한 기초(basis)\n",
    "    - agent 내부의 모든 것들은 완전하게 agent에 의해 제어가능하고, 그 자체를 알고 있다고 가정\n",
    "    - agent 외부의 모든 것들은 불완전하게 agent에 의해 제어되고, 그 자체를 알고 있거나 모를수 있다고 가정\n",
    "    - *policy* : states의 함수로써, agent가 action을 선택하는 확률론적(stochastic) 규칙(rule)\n",
    "    - agent's objective : 시간에 따라 받게 되는 reward의 양을 최대화 하는 것\n",
    "\n",
    "p2:\n",
    "\n",
    "- *Markov decision process* (MDP) : 상기에서 제시한 reinforcement learning의 설정을 잘 정의된 transition probabilities로 공식화하는 것\n",
    "    - *finite* MDP : 유한 개의 state, action, reward로 구성된 MDP\n",
    "    - reinforcement learning의 현재 대부분의 이론들은 finite MDP로 제한됨\n",
    "        - 그러나!, 방법론들과 아이디어들은 더 일반적으로 적용됨\n",
    "\n",
    "p3:\n",
    "\n",
    "- *return* : agent가 maximize하고자 하는 future reward에 대한 함수\n",
    "    - task의 성격과 delayed reward를 discount하고자 하는지에 따라 서로 다른 정의를 갖음\n",
    "        - *episodic tasks* : agent-environment 간 상호 작용이 자연스럽게 각 episode로 분리되는 task를 의미\n",
    "            - undiscounted formulation을 적용하기 적절한 task\n",
    "        - *continuing tasks* : agent-environment 간 상호 작용이 자연스럽게 각 episode로 분리되지 않고 무한히 지속되는 task를 의미\n",
    "            - discounted formulation을 적용하기 적절한 task\n",
    "    - *Unified Notation for Return* : 두 task 모두에 적용할 수 있는 return의 정의\n",
    "\n",
    "p4:\n",
    " \n",
    "- *value functions* : 어떤 특정 policy를 사용할 때, 각 state 혹은 state-action pair에, 해당 state 혹은 state-action pair에서의 expected return을 할당\n",
    "- *optimal value functions* : 각 state 혹은 state-action pair에, 모든 policy에 대해 받을 수 있는 가장 큰 expected return을 할당\n",
    "- *optimal policy* : optimal value function을 value function으로 갖는 policy\n",
    "    - 주어진 MDP에서, state와 state-action pair에 대한 optimal value function은 unique함\n",
    "        - 반면, 다수의 optimal policy가 존재할 수 있음\n",
    "    > Any policy that is *greedy* with respect to the optimal value functions must be an optimal policy\n",
    "- *Bellman optimality equations* : special consistency conditions\n",
    "    - optimal value functions must satisfy and that can be solved for the optimal value functions, from which an optimal policy can be determined with relative ease.\n",
    "\n",
    "p5:\n",
    "\n",
    "- reinforcement learning problem은 초기 agent에게 available한 지식의 수준에 대한 가정에 따라 서로다른 다양한 방법으로 정의됨\n",
    "- *complete knowledge* problem : agent는 environment's dynamics의 완전하고 정확한 모델을 갖음\n",
    "    - 이러한 문제의 MDP에서 모델은 완전한 four-argument dynamics function $ p $(3.2)로 구성됨\n",
    "- *incomplete knowledge* problem : environment에 대한 complete하고 perfect한 모델이 available하지 않음\n",
    "\n",
    "p6:\n",
    "\n",
    "- environment model이 완전하고 정확할지라도, agent가 충분한 computation을 수행하기 어려울 수 있음\n",
    "    - memory available : 중요한 제한 조건\n",
    "    - value function, policy, model에 대한 정확한 approximation을 구축하기 위한 memory가 요구됨\n",
    "    - 현실적으로는 테이블 내 엔트리보다 상당히 많은 수의 state가 존재할 수 있음\n",
    "        - 따라서, approxmiation이 반드시 필요함\n",
    "        \n",
    "p7:\n",
    "\n",
    "- 잘 정의된 optimality 개념은 다양한 learning 알고리즘의 이론적인 속성을 이해하는 방법을 제시함\n",
    "    - 그러나!, 이상적인 개념임..\n",
    "        - reinforcement learning agents는 그 정도에 대한 근사(approximate)만 할 수 잇음\n",
    "- reinforcement learning에서는, 다양한 이유(computational cost, ...)로 optimal solutions를 찾을 수 없으나, 다양한 방법으로 근사해야만 하는 문제에 대해 초점을 맞추고 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
