{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2 Multi-armed bandits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.7 Upper-Confidence-Bound Action Selection\n",
    "\n",
    "p1:\n",
    "\n",
    "- action-value estimates의 정확도에 대한 불확실성이 항상 존재하기 때문에, Exploration이 필요함\n",
    "\n",
    "\n",
    "- greedly actions : estimates된 action-value중 가장 큰 값만 선택 → 다른 action이 더 좋은 결과를 나타낼 수 있음\n",
    "- $ \\epsilon $ - greedy actions : 총 행위 중 $ \\epsilon $ 만큼을 무작위 action으로 선택 → nealry greedy한 action만 선택 가능\n",
    "- **확실한 사실** : 다수의 시행을 거치면 불확실성은 줄어듬!\n",
    "\n",
    "**<font color='red'><center>action-value estimates의 불확실성 정도를 통한 Exploration을 하자!</center></font>**\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "A_t \\doteq argmax_a \\left[ Q_t(a) + c \\sqrt{ \\frac{\\ln {t}}{N_t(a)}} \\right]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "$ \\ln{t} $ : natural logarithm of $ t $\n",
    "\n",
    "$ N_t(a) $ : time $ t $ 까지 action $ a $가 선택된 횟수\n",
    "\n",
    "$ c > 0 $ : exploration의 정도에 대한 제어 변수\n",
    "\n",
    "<br />\n",
    "\n",
    "p2:\n",
    "\n",
    "$c \\sqrt{ \\frac{\\ln {t}}{N_t(a)}}$ : \n",
    "- denominaor : 시행 횟수 $t$에 대한 자연 로그\n",
    "- numerator : 시행 횟수 $t$동안 action $a$가 선택된 횟수\n",
    "- 즉, 많은 시행동안 action $a$가 선택되지 않으면, action $a$가 선택되도록 만듬 → Exploration\n",
    "\n",
    "[그림 1]\n",
    "\n",
    "<br />\n",
    "p3(**page 28**):\n",
    "\n",
    "**<font color='red'>UCB의 단점</font>**\n",
    "- 좀 더 일반적인 reinforcement learning setting (다수의 state가 있는 경우 등) 으로의 확장이 어려움 (approximation 관점으로 확장될 때..)\n",
    "- nonstationary problem에서 적용하기 쉽지 않음 → 시간에 따라 action-value estimates가 달라지기 때문에 uncertanty를 줄일 수 없음!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Bandit Algorithms\n",
    "\n",
    "p1: \n",
    "\n",
    "- 2.1~2.7까지의 방법론 : action-value를 estimate하고, 이 estimate를 이용해서 action을 선택함\n",
    "\n",
    "**<font color='red'><center> action에 대한 선호도 $ H_t(a) $를 학습해서 높은 선호도를 갖는 action를 선택하자!</center></font>**\n",
    "\n",
    "$ H_t(a) $ : numerical **<i>preference</i>** for each action $a$\n",
    "\n",
    "- preference는 reward 관점에서 해석되는 것이 아닌, 각 action 간의 상대적인 선택 확률을 의미!\n",
    "\n",
    "\\begin{align}\n",
    "Pr{A_t=a} \\doteq \\frac {e^{H_t(a)}} {\\sum_{b=1}^{k} e^{H_t(b)}} \\doteq \\pi_t(a)\n",
    "\\end{align}\n",
    "\n",
    "$\\pi_t(a)$ : time $t$에서의 action $a$를 선택할 확률\n",
    "\n",
    "- $t=1$에서 모든 preference $H_1(a)$는 모두 0이므로, 모든 action을 선택할 확률이 같음\n",
    "\n",
    "<br />\n",
    "\n",
    "p2(**page29**):\n",
    "\n",
    "- stochastic gradient asent를 통한 preference update\n",
    "\n",
    "\\begin{align}\n",
    "H_{t+1}(A_t) \\doteq H_t(A_t) + \\alpha (R_t - \\bar{R}_t)(1-\\pi_t(A_t)), \n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "H_{t+1}(a) \\doteq H_t(a) - \\alpha (R_t - \\bar{R}_t)(\\pi_t(a))\n",
    "\\quad \\textrm{ for all }a \\neq A_t\n",
    "\\end{align}\n",
    "\n",
    "$ \\alpha > 0 $ : step-size parameter\n",
    "\n",
    "$ \\bar{R}_t \\in \\mathbb{R} $ : time $ t $까지의 모든 rewards의 평균 - baseline\n",
    "\n",
    "- 만약, reward가 baseline $ \\bar{R}_t $보다 크면? → action $ A_t $를 취할 확률이 높아짐\n",
    "- 만약, reward가 baseline $ \\bar{R}_t $보다 작으면? → action $ A_t $를 취할 확률이 작아짐\n",
    "\n",
    "<br />\n",
    "p3:\n",
    "\n",
    "Figure 2.5 : expected reward를 mean 0가 아닌 mean +4로 설정한 상황에서의 성능 비교\n",
    "\n",
    "- mean +4는 gradient bandit algorithms에 어떠한 영향도 미치지 않음 → baseline이 같이 증가하므로?\n",
    "\n",
    "- 반면, baseline을 제거하면, 성능이 매우 떨어짐!\n",
    "\n",
    "[그림 2]!\n",
    "\n",
    "<br />\n",
    "p4:\n",
    "\n",
    "- 알고리즘 details -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.9 Associative Search (Contextual Bandits)\n",
    "\n",
    "p1:\n",
    "\n",
    "- 본 장에서는 nonassociative task에 대해서만 고려하였음\n",
    "    - action과 state에 대한 연관성이 없음!\n",
    "    - task가 stationary하거나 nonstationary한 경우, single best action을 찾는 시도를 함\n",
    "- general reinforcement learning task\n",
    "    - 하나 이상의 상황(situation)이 있음\n",
    "    - 상황(situation)과 적합한 행동(action)을 매핑하는 policy를 학습하는 것을 목적으로 함\n",
    "- 본 절에서는 본 장에서 다룬 간단한 nonassociative task를 associative task로 확장하는 방법에 대한 논의함\n",
    "\n",
    "p2:\n",
    "\n",
    "- 다수의 k-armed bendit tasks들이 있고, 각 시행 $t$에서 임의의 task에 직면한다고 가정해보자!\n",
    "    - 이 문제를 single, nonstationary k-armed bendit problem으로 생각할 수 있음\n",
    "    - 반면, true action value가 천천히 변하더라도, 본 장에서 언급한 방법론들은 잘 작용하지 않음!\n",
    "- 각 task를 선택할 수 있는 policy가 있다고 가정해보자! \n",
    "    - 각 슬롯 머신에 색상이 부여되어 있고, 각 색상에 따른 task에 본 장에서 언급한 방법론들을 개별적으로 적용할 수 있음\n",
    "    - 색상에 따라 적합한 action을 선택하는 것 → policy\n",
    "\n",
    "> With the right policy you can usually do much better than you could in the absence of any information distinguishing one bandit task from another.\n",
    "\n",
    "p3(**page 32**):\n",
    "- 상기와 같은 예시를 associative search task (a.k.a contexture bandits)라 함\n",
    "    - best action을 찾기 위한 **trial-and-error learning**\n",
    "    - 상황(situation)과 행동(action)의 연관성(**association**)\n",
    "- associative search tasks는 k-armed bendit problem과 full reinforcement learning 사이의 중간에 위치하는 task임\n",
    "    - full reinforcement learning과 같이 policy를 학습\n",
    "    - 각 action은 즉각적인(immediate) reward에만 영향을 미침\n",
    "- full reinforcement learning\n",
    "    - k-armed bendit problem과 associative search tasks에 더해,\n",
    "    - action이 다음, 그다음 상황에서의 reward까지 영향을 모두 미치는 상황을 고려함!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.10 Summary\n",
    "\n",
    "p1:\n",
    "\n",
    "- 본 장에서는 exploration과 exploitation의 균형을 맞추는 간단한 방법들에 대해 소개함\n",
    "- $\\epsilon$-greedy methods\n",
    "    - 전체 시행 중 일부 time step에서 임의의 action 선택\n",
    "- Optimistic initial value\n",
    "    - 초기 action-value 값을 optimistic하게 설정함\n",
    "    - greedy methods임에도 불구하고 exploration을 할 수 있도록 함!\n",
    "- UCB\n",
    "    - 결정적으로 행동 선택(like greedy methods)\n",
    "    - 적게 선택된 행동을 선택하도록 함\n",
    "- Gradient bandit algorithms\n",
    "    - action preference를 estimate함 (action-value를 estimate하지 않음!)\n",
    "    - softmax distribution에 따른 확률적 접근 방법\n",
    "\n",
    "<br />\n",
    "\n",
    "p2: \n",
    "\n",
    "- 어떤 방법론이 가장 좋은가?\n",
    "    - 일반적으로 이러한 질문에 대한 답을 하기는 어려움\n",
    "    - 본 절에서는 본 장에서 실험한 10-amred testbed에서 각각의 방법론들에 대한 정량적 평가를 진행\n",
    "    - But! 각 방법론들은 각기 다른 파라미터들을 가지고 있음\n",
    "    - 각 방법론들의 파라미터를 각기 달리 설정하여 learning curve를 도출하여 \"parameter study\" 진행\n",
    "    - 각 파라미터의 값을 2배씩 늘려서 실험하였으며, log-scale로 그래프 표현\n",
    "    - **그래프를 통해 주목해야 할 점:** 어떠한 paramter가 가장 적합한가? parameter값의 변화에 민감한가? \n",
    "\n",
    "[그림 3]\n",
    "\n",
    "<br />\n",
    "\n",
    "p3:\n",
    "\n",
    "- 앞서 본 장에서 소개한 방법론들은 심플하지만, 저자들은 state of the art라고 간주하고 있음\n",
    "- 더 복잡한 방법론들이 있으나, 복잡성과 가정(assumption)들은 full reinforcement learning에 확장해서 적용하기 어려움\n",
    "- Chapter 5에서는 본 장에서 소개한 방법론들을 이용해서 full reinforcement learning을 해결하는 learning methods들을 소개함\n",
    "\n",
    "<br />\n",
    "\n",
    "p4(**page 33**):\n",
    "\n",
    "- 본 장에서 살펴본 심플한 방법론들은 exploration과 exploitation의 균형 문제에 관한 완전히 만족스러운 해결책은 아님\n",
    "\n",
    "<br />\n",
    "\n",
    "p5:\n",
    "\n",
    "- **<i>Gittins indices</i>** : k-armed bandit problem에서 exploration과 exploitation의 균형 문제를 해결할 수 있는 또 다른 방법\n",
    "    - 본 장에서 제사한 bandit problem보다 일반화된 상황에서 optimal solution을 제공함\n",
    "    - 단점1 : 가능한 문제에 대한 prior distribution이 알려져 있다고 가정함\n",
    "    - 단점2 : full reinforcement learning를 향한 일반화의 어려움 (이론과 계산 문제)\n",
    "    \n",
    "<br />\n",
    "\n",
    "p6:\n",
    "\n",
    "- **<i>Bayesian methods</i>** (a.k.a posterior sampling, Thompson sampling)\n",
    "    - action-value들에 대한 초기 distribution이 알려져 있다고 가정함\n",
    "    - action-value의 distribution을 각 step마다 update함 (즉, true action-value가 stationary하다고 가정함)\n",
    "    - update computation이 매우 복잡함 (<i>conjugate priors</i>라 하는 특정 distributions에서는 간단)\n",
    "    - posterior probability에 따라 행동을 선택함\n",
    "    - 본 장에서 언급한 distribution-free 방법론들과 유사한 성능을 보임\n",
    "\n",
    "<br />\n",
    "\n",
    "p7:\n",
    "\n",
    "> In the Bayesian setting it is even conceivable to compute the optimal balance between exploration and exploitation.\n",
    "\n",
    "> One can compute for any possible action the probability of each possible immediate reward and the resultant posterior distributions over action values. This evolving distribution becomes the information state of the problem.\n",
    "\n",
    "- 1000개 time step에 대해 모든 가능한 action, 모든 action으로 인한 rewards, 모든 가능한 next action, 모든 가능한 next reward 등등을 고려할 수 있다고 가정해 보자!\n",
    "    - 위 가정을 만족할 때, 모든 가능한 사건에 대한 reward들과 probability들이 결정되면, 가능한 action을 선택할 수 있음\n",
    "    - But! 가능한 모든 경우의 수는 action의 개수와 time-step의 수에 따라 extremely rapidly하게 늘어남\n",
    "        - 위 상황 가정에서 action이 2개밖에 없다고 가정할 때, 무려 $2^{2000}$의 경우 수를 고려해야 함\n",
    "    - **approximation approach로 접근 가능함!(현재의 연구 토픽)** → Part 2에서 소개함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
