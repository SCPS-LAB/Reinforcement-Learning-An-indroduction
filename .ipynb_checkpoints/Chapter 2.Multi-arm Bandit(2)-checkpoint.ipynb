{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Incremental Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p1(**page 23**):\n",
    "+ 지금까지 논의한 action-value method는 관찰된 reward의 샘플 평균으로 action value를 추정한다.\n",
    "+ 다음으로 설명할 방법은 이러한 평균을 효율적으로 계산하는 방법에, 특히 일정한 메모리와 일정한 per-time-step, 대해 의문을 제기한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p2(page 24):\n",
    "+ Ri는 이 행동의 i번째 선택 후 받은 보상, Qn은 n-1번의 선택 후 action vale의 추정치\n",
    "\n",
    "\n",
    "\\begin{equation*}\n",
    "Q_n \\doteq \\frac{R_1+R_2+\\ldots+R_{n-1}}{n-1}\n",
    "\\end{equation*}\n",
    "\n",
    "+ 분명한 구현은 모든 보상의 기록을 유지한 다음 추정값이 필요할 때마다 계산을 수행하는 것이나, 시간이 지남에 따라 메모리 및 계산 요구사항 증가"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p3:\n",
    "+ 작고 일정한 계산에 기반한 평균 업데이트 증분 공식\n",
    "\n",
    "+ 이 방식은 reward 계산을 위해 Qn과 n만 필요함\n",
    "\n",
    "![fig_6](https://github.com/SCPSAI/Reinforcement-Learning-An-indroduction/blob/master/images/chapter02/fig_6.PNG?raw=true)\n",
    "\n",
    "eq (2.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p4:\n",
    "\n",
    "![fig_7](https://github.com/SCPSAI/Reinforcement-Learning-An-indroduction/blob/master/images/chapter02/fig_7.PNG?raw=true)\n",
    "+ 위의 식은 일반적으로 다음과 같이 표현 가능\n",
    "\n",
    "\n",
    "\\begin{equation*}\n",
    "NewEstimate\\leftarrow OldEstimate+StepSize[Target-OldEstimate]\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "+ [Target-OldEstimate]는 추정치 오차(error)\n",
    "+ 위의 예시에서는 n번째 reward가 target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p5(page 25):\n",
    "+ StepSize는 time step마다 변경됨, action a에 대한 n번째 reward 처리 시 StepSize 파라미터는 1/n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Tracking a Nonstationary Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p1:\n",
    "+ 지금까지 논의된 평균화 방법은 stationary bandit 문제, 즉 reward 확률이 시간이 지나도 변하지 않는 문제에 적합하다.\n",
    "+ 하지만 우리가 실제 다루는 문제들은 그 확률 분포가 유동적인 nonstationary 강화학습 문제에 직면하며, 이 때 과거의 reward보다 최신 reward에 더 많은 가중치를 부여하는 것이 합리적이다.\n",
    "+ 널리 사용되는 방법 중 하나는 constant step-size 파라미터를 사용하는 방법이 있다.\n",
    "+ 일례로, 위의 수식에서 1/n을 alpha로 대체"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "Q_{n+1} \\doteq Q_n + \\alpha [R_n - Q_n], \\alpha \\in (0,1]\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p2:\n",
    "\n",
    "![fig_8](https://github.com/SCPSAI/Reinforcement-Learning-An-indroduction/blob/master/images/chapter02/fig_8.PNG?raw=true)\n",
    "\n",
    "+ weight의 합이 항상 1이 되기 때문에 weighted average라고 한다.\n",
    "+ 뒤의 weight는 n-i번째까지 관찰된 reward들에 의존\n",
    "+ 1-alpha가 1보다 작으므로 Ri에 주어진 weight는 reward 수가 증가할 수록 지수적으로 감소함 -> exponential recency-weighted average"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p3:\n",
    "+ 때때로 단계마다 step-size 파라미터를 변경하는 것이 편리함\n",
    "+ 단, 수렴 조건 존재\n",
    "\n",
    "\n",
    "\\begin{equation*}\n",
    "\\sum_{n=1}^\\infty \\alpha_n(a) = \\infty      and            \\sum_{n=1}^\\infty \\alpha_n^2(a) < \\infty\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "+ 첫번째 조건은 단계가 초기 조건이나 임의의 변동을 극복할 수 있을 만큼 충분이 크다는 것을 보장해야.\n",
    "+ 두번째 조건은 단계가 수렴할 수 있을 정도로 충분히 작아져야."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p4(page 26)\n",
    "+ 1/n의 경우 두 조건을 만족하지만, alpha의 경우는 두번째 조건을 만족하지 못함(converge되지 못함)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 Optimistic Initial Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p1:\n",
    "+ 지금까지 논의한 모든 방법은 초기 action-value 추정치에 어느 정도 의존함(language of statistics에서 이러한 방법은 초기 추정지에 편향)\n",
    "+ sample average 방법은 모든 동작을 한 번 이상 선택하면 바이어스가 사라지지만, alpha가 constant한 방법의 바이어스는 영구적이지만, (2.6)에 따라 시간이 지나며 감소한다.\n",
    "+ 장점 : 어떤 수준의 보상을 기대할 수 있는지에 대한 prior knowledge 쉽게 제공 가능\n",
    "+ 단점 : 초기 추정치가 사용자가 선택해야 하는 파라미터가 됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p2:\n",
    "+ initial action value는 exploration을 장려하는 간단한 방법으로 사용될 수 있음\n",
    "+ 예로서 10-armed 테스트에서 initial action value를 0이 아닌 5로 설정할 경우, 초기 어떠한 조치를 선택하더라도 보상은 초기 예측치보다 작음\n",
    "+ learner는 다른 action으로 전환하고 보상을 받은 뒤 \"실망\"함\n",
    "+ 결과적으로 추정값이 수렴되기 전에 모든 action들이 시도되므로 greedy action들을 선택하더라도 상당한 양의 exploration을 수행함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p3:\n",
    "\n",
    "![fig_9](https://github.com/SCPSAI/Reinforcement-Learning-An-indroduction/blob/master/images/chapter02/fig_9.PNG?raw=true)\n",
    "\n",
    "+ 처음에는 optimistic 방법이 더 많이 탐색하기 때문에 성능이 저하되지만, 시간이 지남에 따라 탐색이 감소하므로 성능이 향상됨을 보임\n",
    "+ 이 방법은 exploration이 본질적으로 일시적이기 때문에 nonstationary 문제에는 적합하지 않음\n",
    "+ task가 변경되어 exploration의 필요성이 새로워지면 이 방법으로 도움이 되지 않음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
