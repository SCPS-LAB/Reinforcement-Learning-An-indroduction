{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "\n",
    "p1(**page 1**): \n",
    "- 환경과 상호작용을 통해 배우는 방식(아이디어)는 가장 본질적인(?) 것임 (ex, 처음 태어나서 하는 행동들)\n",
    "- 환경과의 상호작용을 통한 학습은 모든 학습 및 지능을 기초로 하는 foundational idea\n",
    " \n",
    "\n",
    "p2:\n",
    "- 본 교재에서는 상호작용을 통해 배우는 computational 접근 방법에 대해 살펴봄\n",
    "- 상호 작용으로부터 goal-directed learning에 초점을 맞추는 reinforcement learning을 중점적으로 살펴봄"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Reinforcement Learning\n",
    "\n",
    "p1:\n",
    "- Reinforcement Learning은 수치적인 reward signal을 최대화(maximize)하기 위해 무엇을 해야 하는지, 즉, 상황(situation)들을 행동(action)들에 사상(mapping)하는 방법을 학습함\n",
    "- agent의 action은 즉각적인 reward뿐만 아니라, 다음 상황에서의 reward, 즉 계속되는 상황에서의 reward에 영향을 미칠 수 있음\n",
    "- RL의 구별되는 주요 특징 : **trial-and-errir** , **delayed rewoard**\n",
    "\n",
    "p2:\n",
    "> Reinforcement learning is simultaneously a problem, a class of solution methods that work well on the problem, and the field that studies this problems and its solution methods.\n",
    "\n",
    "- Reinforcement Learning에서는 **problem**과 **solution method**들을 구별하는 것이 매우 중요함! - 구별을 재대로 못하면 많은 혼란을 야기\n",
    "\n",
    "p3:\n",
    "- Reinforcement Learning에서의 문제(problem)을 dynamical systems theory의 아이디어로부터 공식화함\n",
    "    - optimal control of incompletely-known Markov decision processes : **sensation**, **action**, **goal**\n",
    "- Reinforcement Learning은 MDP로 구성된 문제를 해결하는 모든 방법들을 총칭 (저자 주장)\n",
    "\n",
    "p4(**page 2**):\n",
    "- supervised learning과의 차이점\n",
    "> Supervised learning is learning from a training set of labeled examples provided by a knowledgable external supervisor. Each example is a description of a situation together with a specification—the label—of the correct action the system should take to that situation, which is often to identify a category to which the situation belongs. The object of this kind of learning is for the system to extrapolate, or generalize, its responses so that it acts correctly in situations not present in the training set.\n",
    "\n",
    "- agent가 행동을 취해야 할 상황에서, 즉각적인 혹은 올바른 행동(답안)을 정의하는 것이 불가능한 경우가 종종 발생함\n",
    "\n",
    "p5:\n",
    "- unsupervised learning과의 차이점\n",
    "> Although one might be tempted to think of reinforcement learning as a kind of unsupervised learning because it does not rely on examples of correct behavior, reinforcement learning is trying to maximize a reward signal instead of trying to find hidden structure.\n",
    "- unsupervised learning의 정의:\n",
    "> Unsupervised learning is typically about finding structure hidden in collections of unlabeled data.\n",
    "- reward signal을 maximizing하는 관점에서 큰 차이를 보임\n",
    "\n",
    "p6:\n",
    "- Reinforcement Learning에서 발생하는 문제 중 하나 : **trade-off between exploration and exploitation**\n",
    "- 과거 상황-행동으로부터 최적의 행동을 취하기 위해서는 exploit\n",
    "- 미래 상황에서 더 좋은 행동을 취하기 위해서는 explore\n",
    "\n",
    "p7:\n",
    "- Reinforcement Learning의 주요 특징 중 하나 : 불확실한 환경과 상호작용하는 goal-directed agent의 모든(전체) 문제를 명시적으로 고려함\n",
    "- RL을 제외한 대부분의 ML 방법론들은 전체 문제를 subproblem으로 정의함 (상호 작용 X)\n",
    "\n",
    "p8(**page 3**):\n",
    "> Reinforcement learning takes the opposite tack, starting with a complete, interactive, goal-seeking\n",
    "agent. All reinforcement learning agents have explicit goals, can sense aspects of their environments, and can choose actions to influence their environments.\n",
    "\n",
    "- Reinforcement Learning이 갖는 초기 시점에서의 가정 :\n",
    "    - agent가 직면한 환경에 대한 큰 불확실성에도 불구하고, agent가 작동해야 함\n",
    "    \n",
    "> **When reinforcement learning involves planning**, it has to address the interplay between planning and real-time action selection, as well as the question of how environment models are acquired and improved. \n",
    "\n",
    "> **When reinforcement learning involves supervised learning**, it does so for specific reasons that determine which capabilities are critical and which are not. \n",
    "\n",
    "> For learning research to make progress, important subproblems have to be isolated and studied, but they should be subproblems that play clear roles in complete, interactive, goal-seeking agents, even if all the details of the complete agent cannot yet be filled in.\n",
    "\n",
    "p9:\n",
    "> By a complete, interactive, goal-seeking agent we do not always mean something like a complete organism or robot. These are clearly examples, but a complete, interactive, goal-seeking agent can also be a component of a larger behaving system.\n",
    "\n",
    "- complete, interactive, goal-seeking agent는 완전한 로봇이나 시스템이 될 수도 있고, 로봇이나 시스템의 하위 컴포넌트가 될 수 있음\n",
    "\n",
    "p10:\n",
    "- 현대 Reinforcement Learning의 가장 흥미로운 측면 중 하나는 다른 공학 및 과학 분야와의 실질적이고 유익한 상호 작용임\n",
    "- statistics, optimization, mathmatical subject와의 통합\n",
    "    - \"curse of dimensionality\"를 해결하기 위한 parameterized approximators\n",
    "- 심리학과 신경 과학과의 연관성\n",
    "\n",
    "> Reinforcement learning has also given back, both through a psychological model of animal learning that better matches some of the empirical data, and through an influential model of parts of the brain’s reward system.\n",
    "\n",
    "    --> Chapter 14, 15\n",
    "\n",
    "p11: ???\n",
    "> Finally, reinforcement learning is also part of a larger trend in artificial intelligence back toward\n",
    "simple general principles. Since the late 1960’s, many artificial intelligence researchers presumed that there are no general principles to be discovered, that intelligence is instead due to the possession of a vast number of special purpose tricks, procedures, and heuristics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Examples\n",
    "\n",
    "p1(**page 4**):\n",
    "- Reinforcement Learning의 예시 및 적용 가능 어플리케이션\n",
    "\n",
    "> • A master chess player makes a move. The choice is informed both by planning—anticipating possible replies and counterreplies—and by immediate, intuitive judgments of the desirability of particular positions and moves.\n",
    "\n",
    "> • An adaptive controller adjusts parameters of a petroleum refinery’s operation in real time. The controller optimizes the yield/cost/quality trade-off on the basis of specified marginal costs without sticking strictly to the set points originally suggested by engineers.\n",
    "\n",
    "> • A gazelle calf struggles to its feet minutes after being born. Half an hour later it is running at 20 miles per hour.\n",
    "\n",
    "> • A mobile robot decides whether it should enter a new room in search of more trash to collect or start trying to find its way back to its battery recharging station. It makes its decision based on the current charge level of its battery and how quickly and easily it has been able to find the recharger in the past.\n",
    "\n",
    "> • Phil prepares his breakfast. Closely examined, even this apparently mundane activity reveals a complex web of conditional behavior and interlocking goal–subgoal relationships: walking to the cupboard, opening it, selecting a cereal box, then reaching for, grasping, and retrieving the box. Other complex, tuned, interactive sequences of behavior are required to obtain a bowl, spoon, and milk jug. Each step involves a series of eye movements to obtain information and to guide reaching and locomotion. Rapid judgments are continually made about how to carry the objects or whether it is better to ferry some of them to the dining table before obtaining others. Each step is guided by goals, such as grasping a spoon or getting to the refrigerator, and is in service of other goals, such as having the spoon to eat with once the cereal is prepared and ultimately obtaining nourishment. Whether he is aware of it or not, Phil is accessing information about the state of his body that determines his nutritional needs, level of hunger, and food preferences.\n",
    "\n",
    "p2: \n",
    "- 상기 예시들이 갖는 공통적 특징 : \n",
    "    - active decision-making agent와 불확실한 환경과의 상호 작용\n",
    "    - agent의 행동은 환경의 미래 상태에 영향을 미침\n",
    "    - 올바른 행동의 선택은 **indirect**하고 **delayed** consequences of action들을 고려해야 함\n",
    "    \n",
    "p3:\n",
    "- 행동으로 인한 효과(결과)가 fully predicted되지 않음!\n",
    "- 상기의 모든 예시들은 그에 맞는 목표(goal)들을 포함하고 있음\n",
    "\n",
    "p4:\n",
    "- 상기의 모든 예시들에서, agent는 미래 시점의 성능 향상을 위해 경험(experience)를 사용할 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Elements of Reinforcement Learning\n",
    "\n",
    "p1(**page 5**): Reinforcement Learning의 4가지 주요 서브 요소들 : **a policy, a reward signal, a value function, a model(optionally) of environment**\n",
    "\n",
    "p2: **A policy**\n",
    "- learning agent가 주어신 시간에 행동하는 방식\n",
    "\n",
    "p3: **A reward signal**\n",
    "- Reinforcement Learning 문제의 목표\n",
    "- agent의 유일한 목표는 장기적으로 받는 총 reward를 최대화하는 것\n",
    "- reward signal은 policy를 수정하는 primary basis\n",
    "\n",
    "p4: **A value function**\n",
    "- reward signal이 즉각적인 의미에서 무엇이 좋은지를 의미하는 것이라면, **value function**은 장기적인 관점에서 무엇이 좋은지를 의미\n",
    "> For example, a state might always yield a low immediate reward but still have a high value because it is regularly followed by other states that yield high rewards. Or the reverse could be true. To make a human analogy, rewards are somewhat like pleasure (if high) and pain (if low), whereas values correspond to a more refined and farsighted judgment of how pleased or displeased we are that our environment is in a particular state.\n",
    "\n",
    "p5:\n",
    "- 단순히 value를 예측하는 목적은 reward를 더 많이 얻기 위함임\n",
    "- 결정을 내릴 때 크게 고려해야 할 사항은 value임\n",
    "- Reinforcement Learning에서의 가장 중요한 핵심 컴포넌트는 value를 효ㅕ과적으로 estimating하기 위한 방법임!\n",
    "\n",
    "p6: **A model of environment**\n",
    "> This is something that mimics the behavior of the environment, or more generally, that allows inferences to be made about how the environment will behave.\n",
    "\n",
    "> Methods for solving reinforcement learning problems that use models and planning are called model-based methods, as opposed to simpler model-free methods that are explicitly trial-and- error learners—viewed as almost the opposite of planning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Limitations and Scope\n",
    "\n",
    "p1(**page 6**):\n",
    "> We do not address the issues of constructing, changing, or learning the state signal in this book (other than briefly in Section 17.3). We take this approach not because we consider state representation to be unimportant, but **in order to focus fully on the decision-making issues**. In other words, our main concern is not with designing the state signal, but with deciding what action to take as a function of whatever state signal is available\n",
    "\n",
    "\n",
    "p2:\n",
    "> Most of the reinforcement learning methods we consider in this book are structured around estimating\n",
    "value functions, but it is not strictly necessary to do this to solve reinforcement learning problems.\n",
    "\n",
    "- value function estimate를 하지 않는 reinforcement learning method : \n",
    "    - evolutionary methods (genetic algorithms, genetic programming, simulated annealing, etc.)\n",
    "    - 다수의 고정 policy들을 적용하고, 가장 좋은 reward를 받는 policy 선택\n",
    "    - evalutionary methods의 장점\n",
    "    > We call these evolutionary methods because their operation is analogous to the way biological evolution produces organisms with skilled behavior **even if they do not learn during their individual lifetimes**. If the space of policies is sufficiently small, or can be structured so that good policies are common or easy to find—or if a lot of time is available for the search—then evolutionary methods can be effective. In addition, evolutionary methods have advantages on problems in which the learning agent cannot sense the complete state of its environment.\n",
    "\n",
    "p3:\n",
    "- evaluationary method 단점\n",
    "\n",
    "> Evolutionary methods ignore much of the useful structure of the reinforcement learning problem: they do not use the fact that the policy they are searching for is a function from states to actions; they do not notice which states an individual passes through during its lifetime, or which actions it selects. In some cases this information can be misleading (e.g., when states are misperceived), but more often it should enable more efficient search.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 An Extended Example: Tic-Tac-Toe\n",
    "\n",
    "p1: \n",
    "- Tic-Tac-Toe에 대한 설명\n",
    "\n",
    "> How might we construct a player that will find the imperfections in its opponent’s play and learn to maximize its chances of winning.\n",
    "\n",
    "![fig 1](https://github.com/SCPSAI/Reinforcement-Learning-An-indroduction/blob/master/images/chapter01/fig_1.PNG?raw=true)\n",
    "\n",
    "**fig 1. Tic-Tac-Toe Game**\n",
    "\n",
    "p2(**page 7**):\n",
    "- 문제 해결 방법 1 : \"minimax\" solution - game theory\n",
    "    - is not correct\n",
    "    > it assumes a particular way of playing by the opponent.\n",
    "    \n",
    "    > For example, a minimax player would never reach a game state from which it could lose, even if in fact it always won from that state because of incorrect play by the opponent.\n",
    "\n",
    "- 문제 해결 방법 2 : \"dynamic programming\"\n",
    "    \n",
    "    > can compute an optimal solution for any opponent, but **require as input a complete specification of that opponent, including the probabilities with which the opponent makes each move in each board state**.\n",
    "    \n",
    "    > About the best one can do on this problem is first to learn a model of the opponent’s behavior, up to some level of confidence, and then apply dynamic programming to compute an optimal solution given the approximate opponent model.\n",
    "    \n",
    "\n",
    "p3:\n",
    "\n",
    "- 문제 해결 방법 3 : \"evolutionary method\"\n",
    "    > An evolutionary method applied to this problem would directly search the space of possible policies\n",
    "for one with a high probability of winning against the opponent.\n",
    "\n",
    "\n",
    "p4:\n",
    "\n",
    "- 문제 해결 방법 4 : \"value function\" estimates\n",
    "    - state's value에 대한 probability를 estimate 함\n",
    "    > First we set up a table of numbers, one for each possible state of the game. Each number will be the latest estimate of the probability of our winning from that state. We treat this estimate as the state’s value, and the whole table is the learned value function.\n",
    "\n",
    "\n",
    "p5:\n",
    "\n",
    "- move greedily \n",
    "- move exploratory\n",
    "\n",
    "![fig 2](https://github.com/SCPSAI/Reinforcement-Learning-An-indroduction/blob/master/images/chapter01/fig_2.PNG?raw=true)\n",
    "\n",
    "**fig 2. Tic-Tac-Toe Game**\n",
    "\n",
    "\n",
    "p6(**page 8**): temporal-difference learning method\n",
    "\n",
    "> While we are playing, we **change the values of the states** in which we find ourselves **during the game**.\n",
    "We attempt to make them more accurate estimates of the probabilities of winning.\n",
    "\n",
    "> To do this, we “back up” the value of the state after each greedy move to the state before the move, as suggested by the arrows in Figure 1.1. More precisely, the current value of the earlier state is updated to be closer to the value of the later state. This can be done by moving the earlier state’s value a fraction of the way toward the value of the later state.\n",
    "\n",
    "- $ V(s) \\leftarrow V(s) + \\alpha [V(s') - V(s)] $\n",
    "\n",
    "\n",
    "p7:\n",
    "- 만약 시간에 따라 step-size parameter $ \\alpha$를 줄여주면, optimal policy로 수렴\n",
    "- 만약 step-size parameter $ \\alpha$가 constant하면, 느리게 policy를 변경\n",
    "\n",
    "p8(**page 9**):\n",
    "- evaluationary method와 learn value function의 차이\n",
    "\n",
    "> This example illustrates the differences between evolutionary methods and methods that learn value functions.\n",
    "\n",
    "- evaluationary methods : policy를 고정시킨 후, 많은 게임을 진행한 후 optimal policy 선택\n",
    "    - 게임 진행 중 발생한 사건/상태에 대한 정보는 무시됨\n",
    "    - 게임 진행 중 발생한 여러 사건/상태들이 어떤 영향을 미치는지 알 수 없으므로, 어떤 사건/상태가 승리에 주요한 영향을 미쳤는지 고려 불가\n",
    "- value function methods : 게임 진행 중 상태에 대한 value를 추정하고, 추정된 state value에 따라 policy를 변경\n",
    "    - 게임 진행 중 발생한 사건/상태에 대한 정보를 고려할 수 있음\n",
    "    - 즉, 게임 중 optimize 가능\n",
    "\n",
    "p9:\n",
    "- tic-tac-toe 예시는 Reinforcement Learning의 주요 특징들을 잘 보여줌\n",
    "1. 환경(opponent player)과의 상호작용을 통한 학습 강조\n",
    "2. 명확한 goal이 있고, 올바른 행동을 위해서는 특정 행위의 선택으로 인한 delayed effects를 고려한 계획이나 예측을 요구됨\n",
    "\n",
    "> For example, the simple reinforcement learning player would learn to set up multi-move traps for a shortsighted opponent. It is a striking feature of the reinforcement learning solution that it can achieve the effects of planning and lookahead without using a model of the opponent and without conducting an explicit search over possible sequences of future states and actions.\n",
    "\n",
    "p10:\n",
    "- Reinforcement Learning은 tic-tac-toe 예시 뿐만 아니라 다양한 상황에 적용 가능함\n",
    "    - external adversary가 없는 상황 (game against nature)\n",
    "    - episode가 끝나거나 끝나지 않는 상황 모두\n",
    "    - reward가 아무대서나 발생할 경우\n",
    "\n",
    "p11:\n",
    "- tic-tac-toe example과 달리, state set이 매우 크거나, 무한대일 경우에도 적용 가능\n",
    "    - 예시 1) [Gerry Tesauro (1992, 1995)] backgammon, which has approximately $10^{20}$ states - value function estimate using ANN\n",
    "    > It is in this role that we have the greatest need for supervised learning methods with reinforcement learning. Neural networks and deep learning (Section 9.7) are not the only, or necessarily the best, way to do this.\n",
    "    \n",
    "p12:\n",
    "> In this tic-tac-toe example, learning started with no prior knowledge beyond the rules of the game,\n",
    "but **reinforcement learning by no means entails a tabula rasa view of learning and intelligence.** On the contrary, **prior information can be incorporated into reinforcement learning in a variety of ways** that can be critical for efficient learning. We also had access to the true state in the tic-tac-toe example, whereas reinforcement learning can also be applied when part of the state is hidden, or when different states appear to the learner to be the same.\n",
    "\n",
    "\n",
    "p13:\n",
    "> Finally, the tic-tac-toe player **was able to look ahead and know the states** that would result from each of its possible moves. To do this, it had to have a model of the game that allowed it to foresee how its environment would change in response to moves that it might never make. Many problems are like this, but in others even a short-term model of the effects of **actions is lacking.** Reinforcement learning can be applied in either case. No model is required, but models can easily be used if they are available or can be learned (Chapter 8).\n",
    "\n",
    "p14(**page 10**):\n",
    "> On the other hand, **there are reinforcement learning methods that do not need any kind of environment model at all.** Model-free systems cannot even think about how their environments will change in response to a single action. The tic-tac-toe player is model-free in this sense with respect to its opponent: it has no model of its opponent of any kind. Because models have to be reasonably accurate to be useful, model-free methods can have advantages over more complex methods when the real bottleneck in solving a problem is the difficulty of constructing a sufficiently accurate environment model. Model-free methods are also important building blocks for model-based methods.\n",
    "\n",
    "p15:\n",
    "> Reinforcement learning can be used at both **high and low levels** in a system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 Summary\n",
    "\n",
    "p1:\n",
    "> Reinforcement learning is a **computational approach to understanding and automating goal-directed learning and decision making.**\n",
    "\n",
    "> learning by an agent from **direct interaction with its environment**, without relying on exemplary supervision or complete models of the environment.\n",
    "\n",
    "\n",
    "p2:\n",
    "> Reinforcement learning **uses the formal framework of Markov decision processes** to define the interaction between a learning agent and its environment in terms of **states, actions, and rewards.**\n",
    "\n",
    "p3(**page 11**):\n",
    "> The **concepts of value and value function** are key to most of the reinforcement learning methods that we consider in this book."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.7 Early History of Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 추후 작성..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference\n",
    "\n",
    "1. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
